# Session 55 — 2026-02-26

## Context
- **Phase**: Phase 3 — Partial Observability & Self-Play
- **Branch**: main
- **Previous Sessions**: S53 (root cause analysis + redesign report), S54 (PE design survey)

## Objectives
- [x] Update implementation plan with spec panel fixes
- [x] Implement micro-phase rapid alternation (`_run_micro_phases()`)
- [x] Plumb `evader_v_max`, `capture_bonus`, `n_obstacle_obs` through env stack + CLI
- [x] Add `min_obstacles` floor to `SmoothCurriculumManager`
- [x] Fix evaluation functions to use correct env params
- [x] Smoke test both RA1 and RA2 configurations
- [x] Verify all 687 existing tests still pass

## Work Done

### Parameter Plumbing
- Added `evader_v_max`, `capture_bonus`, `n_obstacle_obs` to `_make_partial_obs_env()`, `AMSDRLSelfPlay.__init__()`, and CLI
- **CRITICAL FIX**: `n_obstacle_obs` was never exposed as CLI arg — agents in `--full_obs` mode were blind to obstacles. Now exposed as `--n_obstacle_obs`.
- Fixed `_evaluate_head_to_head_full_obs()` and `_evaluate_head_to_head()` to pass `evader_v_max` and `n_obstacle_obs` to eval envs

### Curriculum: min_obstacles Floor
- Added `min_obstacles` parameter to `SmoothCurriculumManager`
- `n_obstacles` property now returns `max(self.min_obstacles, ...)` instead of 0 at early distance levels
- Verified: `SmoothCurriculumManager(min_obstacles=2)` returns 2 even before obstacles_after_distance

### Micro-Phase Implementation
Core new method `_run_micro_phases()` (~200 lines) with:
- Persistent environments (created once, reused across all micro-phases)
- n_envs mismatch handling via save/reload pattern after cold-start
- Opponent weight sync via `_sync_opponent_weights_vec()` — copies policy state_dict in-place
- Opponent pool integration via `_resample_pool_opponents_vec()` with 50/50 current/historical
- Progress logging every 10 micro-phases, full eval summary at intervals
- Checkpointing at eval intervals + incremental history saves
- NE-gap curriculum advancement integrated with micro-phase eval loop
- Convergence detection: stops when NE gap < eta

Helper methods:
- `_set_opponents_in_vec_env()`: Set opponents in all sub-envs
- `_sync_opponent_weights_vec()`: Copy weights without recreating envs/adapters
- `_resample_pool_opponents_vec()`: 50% current, 50% pool
- `_save_micro_snapshot()`: Save to opponent pool
- `_save_history_incremental()`: Crash recovery

### Dispatch in run()
- After cold-start + eval, if `micro_phase_steps > 0`, calls `_run_micro_phases()`
- Legacy alternating mode fully preserved

### Smoke Testing
- RA1 config (no curriculum): 4 micro-phases, opponent pool populated, models saved
- RA2 config (curriculum): 4 micro-phases with NE-gap tracking, curriculum level reported
- 224 self-play tests pass, 687 total tests pass (68 pre-existing BarrierNet failures)

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `training/amsdrl.py` | Modified | 8 new __init__ params, `_run_micro_phases()` + 5 helpers, dispatch in `run()`, fixed eval functions |
| `training/curriculum.py` | Modified | Added `min_obstacles` floor to `SmoothCurriculumManager` |
| `scripts/train_amsdrl.py` | Modified | 8 new CLI args, pass-through to `AMSDRLSelfPlay` |
| `docs/RA_implementation_plan.md` | Updated | Marked implemented, corrected launch commands, added spec panel fixes |
| `docs/worklogs/2026-02-26_S55.md` | Created | This worklog |
| `docs/workflow_tracker.md` | Updated | S55 entry |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| micro_phase_steps=2048 (not 4096) | 2048 = exactly 1 PPO rollout (4 envs × 512 steps). Original plan had math error. |
| Weight sync via state_dict copy | Avoids recreating env/adapter stack. Traverses wrapper chain to find PPO model. |
| No rollback in micro-phase mode | Oscillation amplitude too small with 2048-step phases. Rollback was counterproductive (S53). |
| max_total_steps=10M for RA runs | ~2-4 hours on GPU. Enough for 5000 eval checkpoints. |
| Save history at every eval | Crash recovery — can inspect progress even if training crashes. |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| n_obstacle_obs not in CLI | CRITICAL fix: added `--n_obstacle_obs` arg |
| n_envs mismatch after cold-start | save/reload pattern (same as existing _train_phase) |
| eval envs missing evader_v_max | Fixed both eval functions to pass all params |

## Next Steps
- [ ] Push code to GitHub
- [ ] SSH to niro-2, pull latest
- [ ] Kill X, Z1, Z2 (keep V as baseline)
- [ ] Launch RA1 (no curriculum)
- [ ] Launch RA2 (distance curriculum)
- [ ] Monitor for 30-60 min, check stability

## Metrics
- New CLI args: 8
- New methods: 6 (1 core + 5 helpers)
- Files modified: 3
- Lines of new code: ~350
- Smoke tests: 2 (RA1 + RA2 configs)
- Existing tests: 687 pass, 0 new failures
