# Session 28 — 2026-02-22

## Context
- **Phase**: Phase 2.5: BarrierNet Experiment — Session 3 (Actor + PPO Integration)
- **Branch**: main

## Objectives
- [x] Implement `agents/barriernet_actor.py` — BarrierNetActor with MLP + QP layer
- [x] Implement `agents/barriernet_ppo.py` — BarrierNetPPO with rollout buffer and update
- [x] Implement BarrierNetCritic (standard MLP, no QP)
- [x] Implement RolloutBuffer with GAE computation
- [x] Write comprehensive tests (`tests/test_barriernet_actor.py`)
- [x] Verify gradient flow from PPO loss through QP to MLP parameters

## Work Done

### BarrierNetActor
- `BarrierNetActor(nn.Module)`: MLP backbone + tanh-squashed action head + diff QP layer
  - `get_nominal_action()`: MLP → tanh squash to [0, v_max] × [-omega_max, omega_max]
  - `forward()`: obs → MLP → u_nom (+ noise if stochastic) → QP → u_safe
  - Gaussian exploration noise added BEFORE QP (stochastic pre-QP, deterministic post-QP)
  - Log-prob uses Approach A (log pi(u_nom | s) as approximation), same as BarrierNet paper
  - Entropy computed from Gaussian distribution (before QP)

### BarrierNetCritic
- Standard MLP → scalar value estimate, no QP layer needed

### BarrierNetPPO
- `BarrierNetPPOConfig`: Dataclass with all hyperparameters
- `RolloutBuffer`: Stores obs, states, actions, rewards, log_probs, values, dones, obstacles, opponents
  - `compute_returns_and_advantages()`: GAE with configurable gamma/lambda
  - `get_batches()`: Shuffled mini-batch iterator
- `BarrierNetPPO`: Full PPO agent
  - `get_action()`: Inference with optional determinism
  - `update()`: PPO clipped objective with gradient through QP, entropy bonus, grad clipping
  - `save()/load()`: Checkpoint support

### Tests (32 tests, 7 classes)
- `TestBarrierNetActorForward` (7): shapes, bounds, obstacles, opponent, diagnostics
- `TestDeterministicStochastic` (4): consistency, variation, log prob
- `TestGradientFlow` (5): backbone, action head, log_std, PPO loss, no NaN
- `TestBarrierNetCritic` (3): shape, gradients, single sample
- `TestRolloutBuffer` (4): add/length, returns, batches, clear
- `TestBarrierNetPPO` (6): construction, action, update, parameter changes, save/load, deterministic
- `TestQPCorrection` (3): center/wall correction, shape

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `agents/barriernet_actor.py` | Created | BarrierNetActor + BarrierNetCritic |
| `agents/barriernet_ppo.py` | Created | BarrierNetPPO + RolloutBuffer + Config |
| `tests/test_barriernet_actor.py` | Created | 32 tests across 7 test classes |
| `docs/worklogs/2026-02-22_S28.md` | Created | Session worklog |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Log-prob Approach A (use log pi(u_nom)) | Same as BarrierNet paper; exact when QP inactive, small bias when active |
| Gaussian noise before QP, deterministic after | PPO needs stochastic exploration; QP ensures safety deterministically |
| Separate optimizers for actor/critic | Standard PPO practice; critic doesn't need QP gradients |
| GAE in RolloutBuffer (not SB3) | Custom buffer needed for storing states/obstacles for QP constraints |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| None | — |

## Next Steps
- [ ] Session 4: Modify PPO training loop to collect rollouts with BarrierNet actor
- [ ] Session 4: Create training script with env integration
- [ ] Session 5: Train on niro-2 (RTX 5090), compare vs CBF-Beta

## Metrics
- Files created: 3
- Tests added: 32 (total: 436)
- All 436 tests pass
