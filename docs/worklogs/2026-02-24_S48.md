# Session 48 — 2026-02-24

## Context
- **Phase**: Phase 3: Three-Pronged Fix for Level 3+ Collapse
- **Prior**: S47 diagnosed 3 root causes for L3+ evader collapse in Runs H-P

## Objectives
- [x] Fix 1: Dual-criteria curriculum gate with regression (training/curriculum.py)
- [x] Fix 2: Asymmetric training ratio for evader at obstacle levels (training/amsdrl.py)
- [x] Fix 3: CLI args for new params + increased PBRS weight (scripts/train_amsdrl.py)
- [x] Add ~10 new tests for all new features (tests/test_curriculum.py)
- [x] Run full test suite — no regressions (590 pass)
- [x] Sync to niro-2 and launch Run Q
- [x] Fix 4: Equalize terminal rewards (timeout_penalty -50 → -100)
- [x] Fix 5: Add --timeout_penalty CLI flag
- [x] Reward analysis: identified survival_bonus problem
- [x] Launch Run R with corrected reward setup

## Work Done

### Fix 1: Dual-Criteria Curriculum Gate
- Added `min_escape_rate`, `min_phases_per_level`, `regression_floor`, `regression_patience` params to `CurriculumManager.__init__()`
- Added `phases_at_level` and `consecutive_floor_phases` state tracking
- Modified `check_advancement()` to require all 3 criteria: capture_rate, escape_rate, min phases
- Added `check_regression()` method for curriculum regression when evader collapses

### Fix 2: Asymmetric Training Ratio
- Added `evader_training_multiplier` param to `AMSDRLSelfPlay.__init__()`
- Modified `_train_phase()` to multiply evader steps at obstacle levels
- Updated curriculum integration block in `run()` to pass escape_rate and handle regression

### Fix 3: CLI Args
- Added `--evader_multiplier`, `--min_phases_per_level`, `--min_escape_rate` to train_amsdrl.py
- All params wired through to AMSDRLSelfPlay and CurriculumManager

### Fix 4: Equalize Terminal Rewards
- Changed `timeout_penalty` default from -50.0 to -100.0 (matches `capture_bonus=100.0`)
- Updated defaults in `RewardComputer`, `PursuitEvasionEnv`, yaml config
- Added `--timeout_penalty` CLI flag to `train_amsdrl.py`
- Wired through `AMSDRLSelfPlay` → `env_kwargs` → `_make_partial_obs_env` → `RewardComputer`
- Updated all test hardcoded values from -50.0 to -100.0
- Literature confirms 1:1 capture/timeout magnitude is universal (Papers N06, 02, 18, 20)

### Reward Imbalance Analysis
Discovered critical reward structure problem during Run Q monitoring:

**The problem**: `survival_bonus=1.0` shifts visibility reward from [-1,+1] to [0,+2]:
- With survival_bonus: visible=0.0 (no urgency to flee!), hidden=+2.0
- Without survival_bonus: visible=-1.0 (flee!), hidden=+1.0

**Magnitude analysis** (Run Q params, 600-step episode):
- Evader accumulated per-step: up to ±1200 (visibility + survival)
- Evader terminal timeout: +50 (Run Q) or +100 (new default)
- Per-step dominates terminal by 12-24x

**Key insight**: With gamma=0.99 and 600 steps, gamma^600≈0.0024, so terminal rewards are inherently weak. PPO handles this through the advantage function — capture creates a large negative advantage because the agent loses both the terminal penalty AND all future per-step rewards (opportunity cost). So per-step dominance is OK as long as the per-step SIGN is correct.

**Critical finding**: `survival_bonus` only affects L3+ (obstacle levels). At L1-L2, code falls back to zero-sum because `can_use_visibility` requires obstacles. Fix is perfectly targeted.

**Options evaluated**:
1. **Option A** (chosen): survival_bonus=0.0 — minimal fix, correct sign, strong signal
2. **Option K**: vis=0.2, terminal=±200 — cleanest incentive structure but weaker signal
3. **Option E**: vis=0.1, terminal=±100 — terminal dominates but 10x weaker signal
4. Rejected: higher gamma (broad side effects), terminal=±1000 (value function instability)

### Run Q Results (partial, still running)
Run Q validated the three-pronged fix:
- **L1**: Held for 6 phases (was 1 in old runs) — min_phases gate working ✓
- **L2**: Advanced after 4 phases ✓
- **L3**: escape_rate gate blocked premature advancement (escape=0.03 < 0.05) ✓
- **L3→L4**: Evader recovered with 3x training (escape 0.03→0.14) — **first time L3 didn't collapse** ✓
- **L4**: In progress at S16

### Run R Launch
Launched with corrected reward setup:
- `--survival_bonus 0.0` (was 1.0) — THE KEY FIX
- `--pursuer_v_max 1.0` (was 1.1) — equal speeds per literature
- `--timeout_penalty -100.0` (was -50.0 default) — symmetric terminal
- `--max_phases 30` (was 24) — more room with equal speeds

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `training/curriculum.py` | Modified | Dual-criteria gate, regression, phase counting |
| `training/amsdrl.py` | Modified | Asymmetric steps, curriculum call signatures, regression, timeout_penalty wiring |
| `scripts/train_amsdrl.py` | Modified | 6 new CLI args (evader_multiplier, min_phases_per_level, min_escape_rate, timeout_penalty) |
| `tests/test_curriculum.py` | Modified | 14 new tests |
| `envs/rewards.py` | Modified | timeout_penalty default -50 → -100 |
| `envs/pursuit_evasion_env.py` | Modified | timeout_penalty default -50 → -100 |
| `conf/env/pursuit_evasion.yaml` | Modified | timeout_penalty -50 → -100 |
| `tests/test_rewards.py` | Modified | Updated hardcoded timeout_penalty values |
| `tests/test_beta_policy.py` | Modified | Updated hardcoded timeout_penalty values |
| `tests/test_integration.py` | Modified | Updated hardcoded timeout_penalty values |
| `tests/test_self_play.py` | Modified | Updated hardcoded timeout_penalty values |
| `tests/test_training_smoke.py` | Modified | Updated hardcoded timeout_penalty values |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Default min_escape_rate=0 and min_phases=1 for backward compat | Existing tests and non-curriculum runs must not break |
| Multiply evader steps only at obstacle levels | Evader doesn't need extra steps at L1/L2 (no obstacles) |
| PBRS weight 50.0 as CLI param, not code change | Keeps code general, weight tuning is a hyperparameter |
| timeout_penalty -50 → -100 | Literature universally uses equal capture/timeout magnitude (N06, 02, 18, 20) |
| survival_bonus=0.0 for Run R | Removes reward sign error: visible must be negative, not zero |
| Equal speeds (v_max=1.0) for Run R | Literature mostly uses near-equal speeds; 10% advantage was excessive |
| Option A over Option K | Minimal change, strong signal, PPO advantages handle scale via opportunity cost |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| survival_bonus=1.0 made visible=0.0 (no flee urgency) | Set to 0.0 for Run R |
| timeout_penalty was -50 vs capture_bonus +100 (asymmetric) | Equalized to -100 |
| Pursuer 10% faster (1.1 vs 1.0) — literature uses near-equal | Set to 1.0 for Run R |

## Next Steps
- [ ] Monitor Run R — key check: does evader learn obstacle-seeking with survival_bonus=0?
- [ ] If escape rate stays low despite good hiding → escalate to Option K (vis=0.2, terminal=±200)
- [ ] Monitor Run Q to completion for comparison
- [ ] Compare Run Q (survival_bonus=1, speed=1.1) vs Run R (survival_bonus=0, speed=1.0)

## Metrics
- Files changed: 12
- Tests added: 14 (590 total passing)
- Runs launched: 2 (Run Q, Run R)
