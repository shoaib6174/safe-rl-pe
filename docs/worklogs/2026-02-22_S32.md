# Session 32 — 2026-02-22

## Context
- **Phase**: Phase 2.5: BarrierNet Training Fixes + Evaluation + Decision
- **Branch**: main
- **Continuation**: Completing fixes from Session 31 (context overflow)

## Objectives
- [x] Fix test_gradient_to_log_std for non-learnable action_log_std buffer
- [x] Diagnose and fix PPO update gradient flow (fundamental bug)
- [x] Deploy fixed training to niro-2
- [x] Monitor training (287K+ steps, no convergence)
- [x] Run 3-way evaluation comparison
- [x] Write Phase 2.5 decision report

## Work Done

### 1. Fixed action_log_std Test
- Replaced `test_gradient_to_log_std` with `test_log_std_is_fixed_buffer`
- Verifies action_log_std is a non-learnable buffer (prevents std explosion)

### 2. Discovered Critical PPO Gradient Bug
- With fixed std buffer, `test_ppo_style_loss_gradient` also failed
- **Root cause**: During PPO update, the actor's `forward()` was called in stochastic mode, RESAMPLING noise. This means `u_nom - u_nom_mean = noise` (detached), giving no gradient to backbone parameters
- **Fix**: Standard PPO evaluates log probability of STORED actions under CURRENT policy. Added `evaluate_actions(obs, u_nom_stored)` method that computes `log N(u_nom_stored | u_nom_mean_current, sigma)` — gradient flows through `u_nom_mean_current` to backbone
- This also means the QP layer is NOT called during PPO updates (only during rollout inference), making updates ~10x faster

### 3. Implementation Details
- **`BarrierNetActor.evaluate_actions()`**: New method for PPO update log-prob evaluation
- **`RolloutBuffer`**: Added `u_noms` field to store nominal actions during rollout
- **`BarrierNetPPO.update()`**: Uses `evaluate_actions` instead of full forward pass
- **`barriernet_trainer.py`**: Stores `act_info["u_nom"]` in buffer during rollout
- **Entropy bug**: Fixed sum over batch dimension in `evaluate_actions` entropy (logging-only, no gradient impact)

### 4. Training Deployment & Observations
- Deployed ds10 training (distance_scale=10) on niro-2
- Training at 287K steps: rewards flat at -50, no convergence
- Occasional capture spikes (iter 22: reward=26.01) but not learning from them
- QP intervention rate: 85-90% (constant), infeasibility: 0%
- Policy loss: -0.02 (gradients flowing but ineffective)

### 5. 3-Way Evaluation (100 episodes, 2 obstacles)
Created `scripts/evaluate_barriernet.py` with 3-way comparison:

| Metric | BarrierNet PPO | Baseline PPO | Baseline+Filter |
|--------|:-:|:-:|:-:|
| Capture Rate | 3.0% | 4.0% | 1.0% |
| Safety Violation Rate | 7.42% | 4.05% | **2.91%** |
| QP Intervention Rate | 87.5% | N/A | **0.1%** |
| Inference Time | 13.49 ms | 0.18 ms | 0.40 ms |

### 6. Phase 2.5 Decision Report
- Wrote `results/phase2_5/decision_report.md`
- **Recommendation**: Post-hoc VCP-CBF safety filter (train unconstrained, deploy with filter)
- Post-hoc filter achieves best safety (2.91%) with minimal intervention (0.1%)
- BarrierNet's 87.5% intervention rate prevents exploration/convergence

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `agents/barriernet_actor.py` | Modified | Added `evaluate_actions()`, fixed entropy bug |
| `agents/barriernet_ppo.py` | Modified | Added `u_noms` to buffer, use `evaluate_actions` in update |
| `training/barriernet_trainer.py` | Modified | Store `u_nom` in buffer during rollout |
| `tests/test_barriernet_actor.py` | Modified | Fixed gradient tests for buffer std + evaluate_actions |
| `scripts/train_barriernet.py` | Modified | Added `--distance-scale` flag |
| `scripts/evaluate_barriernet.py` | Created | 3-way evaluation: BarrierNet vs Baseline vs Baseline+Filter |
| `evaluation/comparison_framework.py` | Modified | Fix obs dim truncation, filter_action tuple unpacking |
| `results/phase2_5/decision_report.md` | Created | Phase 2.5 decision report |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Fixed std (buffer) + evaluate_actions | Learnable std explodes due to QP-noise correlation. evaluate_actions provides proper gradient path |
| Post-hoc filter over BarrierNet | Best safety (2.91% violations), minimal intervention (0.1%), fast inference (0.40 ms) |
| Keep BarrierNet code as artifact | Valuable research infrastructure even though training doesn't converge |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| PPO update had no gradient path with fixed std | Added evaluate_actions method |
| BarrierNet rewards flat at -50 after 287K steps | QP 87.5% intervention prevents exploration |
| filter_action returns tuple, not unpacked | Fixed tuple unpacking in SB3EvalAgent |
| Baseline obs_dim mismatch (14 vs 18) | Added obs truncation in SB3EvalAgent |
| GPU contention slows eval+training | Paused training during evaluation |

## Next Steps
- [x] Phase 2.5 decision: Post-hoc VCP-CBF filter recommended
- [ ] Kill BarrierNet training on niro-2 (no convergence expected)
- [ ] Phase 3: Multi-agent self-play with post-hoc safety filter
- [ ] Re-train baseline with obstacles in observation (obs_dim=18)

## Metrics
- Files changed: 8
- Tests: 478 pass (32 barriernet, all passing)
- Commits: d310d33, c21fb81, 1b41937, b6a4c41, 4271217, ef1db9f, 9856295
