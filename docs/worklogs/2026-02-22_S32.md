# Session 32 — 2026-02-22

## Context
- **Phase**: Phase 2.5: BarrierNet Training Fixes (gradient flow + std explosion)
- **Branch**: main
- **Continuation**: Completing fixes from Session 31 (context overflow)

## Objectives
- [x] Fix test_gradient_to_log_std for non-learnable action_log_std buffer
- [x] Diagnose and fix PPO update gradient flow (fundamental bug)
- [x] Deploy fixed training to niro-2
- [ ] Monitor training to 2M steps and evaluate

## Work Done

### 1. Fixed action_log_std Test
- Replaced `test_gradient_to_log_std` with `test_log_std_is_fixed_buffer`
- Verifies action_log_std is a non-learnable buffer (prevents std explosion)

### 2. Discovered Critical PPO Gradient Bug
- With fixed std buffer, `test_ppo_style_loss_gradient` also failed
- **Root cause**: During PPO update, the actor's `forward()` was called in stochastic mode, RESAMPLING noise. This means `u_nom - u_nom_mean = noise` (detached), giving no gradient to backbone parameters
- **Fix**: Standard PPO evaluates log probability of STORED actions under CURRENT policy. Added `evaluate_actions(obs, u_nom_stored)` method that computes `log N(u_nom_stored | u_nom_mean_current, sigma)` — gradient flows through `u_nom_mean_current` to backbone
- This also means the QP layer is NOT called during PPO updates (only during rollout inference), making updates ~10x faster

### 3. Implementation Details
- **`BarrierNetActor.evaluate_actions()`**: New method for PPO update log-prob evaluation
- **`RolloutBuffer`**: Added `u_noms` field to store nominal actions during rollout
- **`BarrierNetPPO.update()`**: Uses `evaluate_actions` instead of full forward pass
- **`barriernet_trainer.py`**: Stores `act_info["u_nom"]` in buffer during rollout
- **Entropy bug**: Fixed sum over batch dimension in `evaluate_actions` entropy (logging-only, no gradient impact)

### 4. Training Deployment
- Killed old training processes on niro-2
- Started new training with fixed gradient flow (PID 1984581)
- Training parameters: 2M steps, 2048 rollout, CUDA, entropy_coeff=0.001
- Training speed: ~7.3s/iteration, ~2-3h estimated total

### 5. Training Observations (125K steps)
- **std_v=0.301, std_w=0.497**: Fixed, no longer exploding
- **Critic loss**: Decreasing from 14.5 → 6-8 (learning)
- **Policy loss**: -0.01 to -0.02, non-zero (gradients flowing)
- **Rewards**: Flat at -50 (all episodes timeout)
- **QP intervention rate**: ~85% (high, expected for untrained policy)
- **Infeasibility rate**: 0% (QP always feasible)

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `agents/barriernet_actor.py` | Modified | Added `evaluate_actions()`, fixed entropy bug |
| `agents/barriernet_ppo.py` | Modified | Added `u_noms` to buffer, use `evaluate_actions` in update |
| `training/barriernet_trainer.py` | Modified | Store `u_nom` in buffer during rollout |
| `tests/test_barriernet_actor.py` | Modified | Fixed gradient tests for buffer std + evaluate_actions |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Fixed std (buffer) + evaluate_actions | Learnable std explodes because QP corrections create positive noise-reward correlation. evaluate_actions provides proper gradient path through MLP backbone |
| No QP in PPO update | With Approach A (log pi of u_nom), QP gradient not needed during update. QP only runs during rollout for safe action execution |
| Entropy_coeff=0.001 | Even with fixed std, entropy bonus is constant, so coeff doesn't affect gradients. Kept small for clean loss logging |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| PPO update had no gradient path with fixed std | Added evaluate_actions method with stored nominal actions |
| SSH nohup $! not expanding | Used single quotes and checked ps instead |
| Python not found on niro-2 via nohup | Used .venv/bin/python -u for unbuffered output |
| Rewards flat at -50 after 125K steps | Monitoring — may need more steps or reward shaping |

## Next Steps
- [ ] Monitor training to completion (~2M steps, ~2-3h)
- [ ] If rewards don't improve by 500K: investigate reward structure
- [ ] Run evaluation comparison (BarrierNet vs baseline PPO)
- [ ] Generate Phase 2.5 decision report

## Metrics
- Files changed: 4
- Tests: 478 pass (32 barriernet, all passing)
- Commits: d310d33 (gradient fix), c21fb81 (entropy fix)
