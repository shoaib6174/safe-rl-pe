# Session 53 — 2026-02-26

## Context
- **Phase**: Phase 3 — Partial Observability & Self-Play
- **Branch**: main

## Objectives
- [x] Deep root cause analysis of self-play collapse across all 4 runs
- [x] Research state of the art in stabilizing competitive self-play (30+ papers)
- [x] Analyze game-theoretic properties of our PE environment
- [x] Critically assess reward design, game setup, and curriculum
- [x] Produce comprehensive redesign report

## Work Done

### Root Cause Analysis
Identified three interacting failures causing all observed training instabilities:

1. **The game has no meaningful equilibrium at Level 1** — Equal speeds (1.0/1.0) in bounded arena with no obstacles means pursuer always wins. No 1v1 PE paper uses equal speeds. NE capture rate ~55-75%.

2. **The reward provides no useful signal** — Terminal-to-shaping ratio is 629:1 to 1136:1. Distance shaping is functionally irrelevant. At equal speeds, optimal play produces zero per-step shaping. Evader has no independent signal at L1.

3. **The self-play protocol amplifies instability** — 100K-500K step frozen phases are 10-50× longer than any successful implementation (Unity ML: 10K, OpenAI Five: continuous). No historical opponent sampling. Rollback is counterproductive.

### Literature Research
Surveyed 30+ papers/resources across: self-play stability, population methods, PE game design, reward engineering, curriculum learning. Key finding: every successful self-play system uses either continuous training against a mix of opponents or population-based methods. None use long frozen phases.

### Game-Theoretic Analysis
- Arena is actually 20×20m (not 10×10 as sometimes referenced in docs)
- Unicycle dynamics with stop-and-turn capability (not Dubins car)
- Equal speed means zero closing rate in open field — pursuer relies entirely on wall cornering
- Episode is 1200 steps (60s) — on the long side vs literature (most use 25-30s)
- Capture radius 0.5m is standard

### Reward Design Analysis
- Distance shaping: 0.00177 per step max, ~0.1 per episode — negligible vs ±100 terminal
- Equal-speed dead zone: both agents get ~0 per-step when playing optimally
- Visibility reward discontinuity: 0 per-step (L1/L2) → ±1.0 per-step (L3+) = 12× scale change at curriculum transition

### Curriculum Analysis
- Curriculates over wrong dimension (distance barely affects difficulty at equal speeds)
- Obstacle step function (0 → 3) is the largest distribution shift
- Advancement criterion (capture_rate > 0.70) requires domination, contradicts balanced play goal
- Ordering is backwards: should start WITH obstacles (strategy enablers), not without

### Redesign Report
Produced comprehensive report at `docs/S53_redesign_report.md` covering:
- Game setup: evader speed 1.15×, obstacles from day 1, 600-step episodes
- Rewards: terminal ±10, survival bonus 0.02, visibility weight 0.1
- Self-play: 4096-step micro-phases, 50/50 opponent pool, no rollback
- Curriculum: try no-curriculum first, fall back to distance-only
- Experiment plan: 2 primary runs + 3 ablations

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `docs/S53_redesign_report.md` | Created | Comprehensive redesign report (18 references) |
| `docs/worklogs/2026-02-26_S53.md` | Created | This worklog |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Evader speed advantage (1.15×) | Standard in PE literature; creates genuine mixed equilibrium |
| Obstacles from day 1 | Eliminates distribution shift; gives evader strategic depth |
| Terminal ±10 (was ±100) | Rebalances terminal:shaping ratio from 629:1 to 63:1 |
| 4096-step micro-phases | Matches successful implementations; eliminates frozen-opponent exploitation |
| 50/50 opponent pool | Standard practice (OpenAI Five 80/20, Unity 50/50) |
| Remove rollback | Counterproductive with micro-phases; weakens struggling agent |
| Kill X, Z1, Z2; keep V | V is useful baseline; others have same fundamental problems |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| Game equilibrium doesn't exist at L1 with equal speeds | Speed advantage + obstacles from day 1 |
| Reward provides no dense signal | Rebalance terminal, add survival bonus, enable visibility |
| Self-play protocol amplifies instability | Micro-phases + opponent pool |

## Next Steps
- [ ] Implement micro-phase training (env caching, opponent sync)
- [ ] Add evader speed advantage parameter
- [ ] Rebalance rewards (terminal ±10, survival 0.02, visibility 0.1)
- [ ] Ensure obstacles always present (min_obstacles floor)
- [ ] Reduce episode length to 600 steps
- [ ] Kill X, Z1, Z2 on niro-2
- [ ] Launch RA1 (no curriculum) and RA2 (distance curriculum)

## Metrics
- Research papers/resources reviewed: 30+
- Root causes identified: 3 major (12 sub-causes)
- Design changes proposed: 10
- Estimated implementation: 7-9 hours
