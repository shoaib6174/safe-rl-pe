# Session 44 — 2026-02-24

## Context
- **Phase**: Phase 3 — Stage 3 obstacle-level collapse research
- **Previous**: S43 implemented opponent pool. Runs H-O all collapsed at L3+ obstacle levels.

## Objectives
- [x] Review all experimental evidence (Runs H-O) and diagnosis
- [x] Research principled solutions from literature and existing paper collection
- [x] Analyze sensory feasibility (can evader detect obstacles?)
- [x] Evaluate arena size impact on obstacle-level performance
- [x] Write research report with proposed fix
- [ ] Implement PBRS obstacle-seeking (deferred to next session)

## Work Done

### 1. Deep Review of Level 3+ Collapse

Reviewed the full `docs/stage3_diagnosis.md` (8 runs: H through O), `envs/rewards.py`, `envs/pursuit_evasion_env.py`, `envs/partial_obs_wrapper.py`, `envs/sensors.py`, and `training/amsdrl.py`.

**Root cause confirmed**: The evader needs three capabilities to use obstacles:
1. Time (prep phase provides) ✓
2. Incentive (visibility reward provides) ✓
3. Direction toward obstacles → **MISSING**

### 2. Literature Research

Searched existing paper collection for solutions:
- **Paper N15 (RMARL-CBF-SAM)**: Uses PBRS form `r_{s,i} = γ·h(s') - h(s)` with proof of policy preservation (Proposition 1)
- **Ng et al. (ICML 1999)**: PBRS is the ONLY additive shaping that preserves optimal policy
- **Devlin & Kudenko (AAMAS 2012)**: PBRS preserves Nash equilibria in multi-agent settings
- **Baker et al. (ICLR 2020, OpenAI H&S)**: Pure visibility reward works but needs billions of steps
- **Wiewiora (JAIR 2003)**: PBRS equivalent to Q-value initialization

**Conclusion**: Potential-Based Reward Shaping with Φ(s) = -d_nearest_obstacle is the principled fix.

### 3. Sensory Feasibility Analysis

Investigated whether the evader can even *sense* obstacles to act on the PBRS gradient:

- Evader observes obstacles ONLY through **lidar** (36 rays, max_range=5.0m)
- `n_obstacle_obs` is NOT passed to env — defaults to 0 (no explicit obstacle features)
- In **10×10 arena**: 3 obstacles in 100m², average d_nearest ~2-3m → lidar usually detects ✓
- In **20×20 arena**: 3 obstacles in 400m², average d_nearest ~5-8m → lidar often can't reach ✗

**Critical finding**: PBRS is computed from true state (standard for reward shaping), but the policy needs sensory input to correlate with the gradient. In 10×10, lidar range is sufficient. In 20×20, it is not — will need increased lidar range or more obstacles when scaling.

### 4. Arena Size Impact Assessment

**Q: Does field size affect current performance?**
A: Yes, enormously. 10×10 has 4× higher obstacle density, lidar covers ~50% vs ~12% of arena, and prep phase travel (5m) covers half the arena vs 25%.

**Q: Should we run O config on 20×20?**
A: No. Makes things strictly worse — same reward plateau + worse sensory coverage + more compute. Fix the reward at 10×10 first.

### 5. Research Report

Wrote comprehensive report at `docs/research_pbrs_obstacle_seeking.md` covering:
- Problem statement with all 8 runs' evidence
- Literature review (5 papers)
- Proposed PBRS solution with formal justification
- Sensory feasibility analysis (new Section 5)
- Arena size considerations
- Alternatives considered and rejected (5 approaches)
- Implementation plan
- Expected outcomes and failure modes

## Files Changed

| File | Action | Description |
|------|--------|-------------|
| `docs/research_pbrs_obstacle_seeking.md` | Created | Full research report (9 sections, 5 references) |
| `docs/worklogs/2026-02-24_S44.md` | Created | This worklog |

## Decisions Made

| Decision | Rationale |
|----------|-----------|
| Use PBRS (Φ = -d_nearest) not spawn-near-obstacles | PBRS is policy-preserving (Ng 1999), spawn changes the MDP |
| Use PBRS not RND/intrinsic motivation | PBRS is targeted at obstacles; RND explores everything |
| Don't run 20×20 yet | Fix reward at 10×10 first; lidar range insufficient for 20×20 |
| Keep prep phase + visibility + PBRS together | Each addresses a different missing capability (time, incentive, direction) |
| Weight w_obs_approach=10.0 initially | Matches pursuer's distance_scale; intentionally small to break plateau without dominating |

## Issues & Blockers

| Issue | Resolution |
|-------|------------|
| Lidar range (5m) insufficient for 20×20 | Defer to scaling session — increase to 10-15m when moving to 20×20 |
| PBRS weight tuning unknown | Start with w=10, prepared to try w=50 or w=100 if too weak |

## Next Steps
- [ ] Implement PBRS in `envs/rewards.py` (add `w_obs_approach` param + nearest-obstacle distance tracking)
- [ ] Thread through `training/amsdrl.py` and `scripts/train_amsdrl.py`
- [ ] Add tests for PBRS reward computation
- [ ] Kill Run O on niro-2 (PID 150319, already collapsed)
- [ ] Launch Run P with PBRS + visibility + prep phase on niro-2
- [ ] Monitor Run P for L3+ behavior

## Metrics
- Files created: 2
- Papers referenced: 5
- Alternatives evaluated: 5
- Implementation: deferred to next session
