# Session 33 — 2026-02-22

## Context
- **Phase**: Phase 2.5 Final Evaluation
- **Branch**: main

## Objectives
- [x] Retrain baseline PPO WITH obstacles (2 obstacles, obs_dim=18)
- [x] Run final 3-way evaluation (200 episodes) on niro-2
- [x] Update Phase 2.5 decision report with final results
- [x] Kill BarrierNet training on niro-2

## Work Done

### Baseline PPO Retraining with Obstacles
- Added obstacle parameters to Hydra config (`conf/env/pursuit_evasion.yaml`)
- Updated `training/utils.py` `make_pe_env` to pass obstacle params
- Fixed `.gitignore`: `env/` was matching `conf/env/`, changed to `/env/`
- Trained on niro-2: `python scripts/train.py env.n_obstacles=2 env.n_obstacle_obs=2 total_timesteps=1000000 seed=42`
- Result: ep_rew_mean=100 at convergence, 14 minutes training time
- Model saved to niro-2: `models/local_42/final_model.zip`

### Final 3-Way Evaluation (200 episodes)
- Ran on niro-2 with new obstacle-trained baseline
- Results:

| Metric | BarrierNet PPO | Baseline PPO | Baseline + Filter |
|--------|---------------|-------------|-------------------|
| Capture Rate | 2.0% | **100.0%** | 91.5% |
| Mean Reward | -47.08 | **100.30** | 87.54 |
| Safety Violations | 7.56% | **6.07%** | 6.84% |
| Intervention Rate | 88.2% | N/A | 44.5% |
| Inference Time | 5.42ms | 0.18ms | 0.46ms |

### Key Insights
1. Standard PPO with obstacle observations achieves 100% capture — no special safety layer needed for training
2. Post-hoc VCP-CBF filter (alpha=1.0, d=0.1) is too aggressive: 44.5% intervention reduces capture from 100% to 91.5%
3. Baseline without filter has LOWER violation rate (6.07%) than with filter (6.84%) — learned implicit obstacle avoidance
4. BarrierNet confirmed non-viable: 2% capture, 88% intervention, worst safety

### Decision Report Update
- Updated `docs/phases/phase2_5_decision_report.md` from Interim to Final status
- Updated recommendation: train PPO with obstacles, optionally add tuned post-hoc filter
- Added new section on filter parameter tuning needs

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `conf/env/pursuit_evasion.yaml` | Modified | Added obstacle params (prev session) |
| `training/utils.py` | Modified | Pass obstacle params to env factory (prev session) |
| `.gitignore` | Modified | Fixed env/ pattern to /env/ (prev session) |
| `docs/phases/phase2_5_decision_report.md` | Modified | Final results, updated recommendations |
| `docs/worklogs/2026-02-22_S33.md` | Created | This worklog |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Train PPO with obstacles for Phase 3 | 100% capture, 6% violations, 14 min training — simple and effective |
| Post-hoc filter needs tuning before use | alpha=1.0 too aggressive, 44.5% intervention hurts performance |
| BarrierNet abandoned | Fundamental exploration-safety conflict, 2% capture after 324K steps |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| Filter too aggressive (44.5% intervention) | Need to tune alpha (0.3-0.5) and d (0.05) before deployment |

## Next Steps
- [ ] Phase 3: Self-play training with obstacle-aware PPO
- [ ] Tune VCP-CBF filter parameters (alpha, d) for lower intervention rate
- [ ] Copy trained model from niro-2 to local for Phase 3 baseline

## Metrics
- Files changed: 2 (decision report, worklog)
- Evaluation episodes: 200 (3-way comparison)
