# Session 35 — 2026-02-22

## Context
- **Phase**: Phase 2.5 (CBF Safety Filter Tuning)
- **Branch**: main

## Objectives
- [x] Research solutions for overly aggressive VCP-CBF filter (44.5% intervention, increases violations)
- [x] Implement epsilon safety margin in QP constraint
- [x] Implement Discrete-Time CBF (DCBF) filter mode
- [x] Add CBF-RL dual reward shaping to SafetyRewardComputer
- [x] Create parameter sweep script and run 14-config experiment
- [x] Document all changes for paper writing

## Work Done

### 1. Research: CBF Filter Tuning
- Investigated 5 approaches from recent literature: CBF-RL reward shaping, DCBF formulations, CSRL for pursuit-evasion, adaptive CBF parameters
- Created comprehensive research report: `claudedocs/research_cbf_filter_tuning.md`
- Identified 4 priority fixes ranked by implementation effort

### 2. Epsilon Safety Margin (Priority 2)
- Added `epsilon` parameter to `solve_cbf_qp()` in `safety/vcp_cbf.py`
- CBF constraint becomes `a_v*v + a_omega*omega + alpha*h >= epsilon` (was `>= 0`)
- Propagated through `VCPCBFFilter.__init__` and `filter_action`
- Purpose: Buffer against discrete-time overshoot near constraint boundaries

### 3. Discrete-Time CBF (Priority 4 → turned out to be the key fix)
- Added `dcbf_filter_action()` method to `VCPCBFFilter`
- Implements: `h(x_{k+1}) >= (1-gamma) * h(x_k)`
- Linearized form: `dt*a_v*v + dt*a_omega*omega + gamma*h >= 0`
- Passes `alpha=1.0` to QP solver with scaled coefficients

### 4. CBF-RL Dual Reward Shaping (Priority 3)
- Enhanced `SafetyRewardComputer` in `envs/rewards.py`
- Added Mode 2: `r_cbf = -w_cbf_penalty * max(0, -cbf_condition_value) - w_intervention * (1 - exp(-||u_diff||^2/sigma^2))`
- New parameters: `w_cbf_penalty`, `w_intervention`, `sigma_sq`
- Ready for Phase 3 retraining; not yet activated

### 5. Parameter Sweep Experiment
- Created `scripts/sweep_filter_params.py` — tests 14 configurations
- Configurations: no filter, CT alpha in {0.1, 0.3, 0.5, 0.7, 1.0}, CT+epsilon combos, DCBF gamma in {0.05, 0.1, 0.2, 0.5}
- Ran 100 episodes per config (1400 total episodes)
- Generated 4-panel bar chart and Pareto frontier plot

### 6. Key Results
**DCBF gamma=0.2 dominates all configurations:**
| Config | Capture | Violations | Intervention | Reward |
|--------|---------|------------|--------------|--------|
| No Filter | 99.0% | 6.49% | 0.0% | 98.8 |
| CT alpha=1.0 (old) | 95.0% | 7.72% | 45.9% | 92.8 |
| **DCBF gamma=0.2** | **100.0%** | **3.57%** | **6.6%** | **100.3** |
| DCBF gamma=0.5 | 100.0% | 2.96% | 5.3% | 100.3 |

Continuous-time CBF is fundamentally broken — lower alpha paradoxically increases intervention (alpha=0.1: 99.7% intervention). The DCBF formulation properly handles the discrete timestep.

### 7. Documentation
- Updated `claudedocs/research_cbf_filter_tuning.md` with experimental validation (Section 4)
- Updated `docs/phases/phase2_5_decision_report.md` with sweep findings, resolved filter status
- Created baseline comparison visualizations (23 plots in `results/visualizations/`)

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `safety/vcp_cbf.py` | Modified | Added epsilon param to `solve_cbf_qp`, added `dcbf_filter_action()` method |
| `envs/rewards.py` | Modified | Added CBF-RL dual reward to `SafetyRewardComputer` |
| `scripts/sweep_filter_params.py` | Created | 14-config parameter sweep script |
| `scripts/visualize_baseline_comparison.py` | Created | 7-type baseline comparison visualization |
| `results/filter_sweep/sweep_results.txt` | Created | Sweep results table |
| `results/filter_sweep/sweep_comparison.png` | Created | 4-panel comparison bar chart |
| `results/filter_sweep/pareto_frontier.png` | Created | Capture vs intervention scatter |
| `results/visualizations/` | Created | 23 baseline comparison plots |
| `claudedocs/research_cbf_filter_tuning.md` | Created+Updated | Research report + experimental validation |
| `docs/phases/phase2_5_decision_report.md` | Modified | Added sweep results, resolved filter status |
| `docs/worklogs/2026-02-22_S35.md` | Created | This worklog |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| DCBF gamma=0.2 as default filter | Dominates all other configurations on ALL metrics (capture, safety, intervention) |
| Keep CT-CBF code but recommend DCBF | CT code useful for comparison/paper, but DCBF should be used in practice |
| CBF-RL reward ready but not activated | Available for Phase 3 retraining if further safety improvement needed |
| Epsilon margin not recommended standalone | Makes CT-CBF even more conservative; only useful combined with DCBF |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| CT alpha sweep showed paradoxical behavior | Lower alpha increases intervention due to constraint linearization issues — resolved by switching to DCBF |
| CT-CBF fundamentally broken for dt=0.05s | DCBF formulation properly handles discrete timesteps |

## Next Steps
- [ ] Commit and push all filter tuning changes
- [ ] Begin Phase 3: Self-play training with DCBF filter
- [ ] Consider CBF-RL reward during Phase 3 training

## Metrics
- Files changed: 11
- New scripts: 2
- Configurations tested: 14
- Total episodes evaluated: 1400
- Documents updated: 3
