# Session 52 — 2026-02-26

## Context
- **Phase**: Phase 3: Partial Observability & Self-Play
- **Branch**: main
- **Prereq**: S51 identified 8 root causes for evader L2 collapse, recommended 9 fixes in 3 tiers

## Objectives
- [x] Implement Tier 1 fixes: bilateral rollback, evader-first at new levels
- [x] Implement Tier 2 fixes: warm-start evader, mixed-level replay, smooth curriculum
- [x] Launch training Runs V+W (Tier 1+2) and X+Y (Tier 1+2+3) — 4 runs total
- [x] Implement Tier 3: EWC (Fix 5) and RND (Fix 9) — catastrophic forgetting prevention and exploration
- [x] Add tests for all new features (40 tests)
- [x] Update documentation

## Work Done

### Tier 1+2 Fixes (from first part of session)
- **Bilateral rollback**: `--bilateral_rollback` flag in AMSDRLSelfPlay
- **Evader-first on advance**: `--evader_first_on_advance` forces evader training at new curriculum levels
- **Warm-start evader**: `--warm_start_evader` with `--warm_start_timesteps` restores and pre-trains evader at transitions
- **Mixed-level replay**: `--mixed_level_ratio 0.2` keeps 20% of envs at previous level distances
- **Smooth curriculum**: `--smooth_curriculum` with `--smooth_curriculum_increment 0.5` for continuous difficulty scaling
- All Tier 1+2 CLI args added to `scripts/train_amsdrl.py`

### Training Runs V, W, X, Y Launched
- **Run V**: Equal speed (pursuer=1.0, evader=1.0), Tier 1+2 only, seed=47
- **Run W**: Pursuer 95% speed (pursuer=0.95, evader=1.0), Tier 1+2 only, seed=47
- **Run X**: Equal speed + Tier 3 (EWC λ=1000, RND coef=0.1), seed=47
- **Run Y**: Pursuer 95% + Tier 3 (EWC λ=1000, RND coef=0.1), seed=47
- All 4 running in background on local machine

### Tier 3: EWC (Elastic Weight Consolidation) — `training/ewc.py`
- `EWCRegularizer` class with:
  - `snapshot(model, obs_batch)`: saves theta_star, computes diagonal Fisher info via policy log-prob gradients
  - `register_hooks(model)`: injects `lambda * F * (theta - theta_star)` into gradients via PyTorch hooks
  - `remove_hooks(handles)`: cleans up after training
  - `penalty(model)`: scalar penalty for logging
- Integrates with SB3 PPO without subclassing — hooks inject EWC gradient during standard backprop
- Triggered on curriculum advancement in `AMSDRLSelfPlay.run()` via `_ewc_snapshot_evader()`

### Tier 3: RND (Random Network Distillation) — `envs/rnd.py`
- `RunningMeanStd`: Welford's online algorithm for reward normalization
- `RNDModule(nn.Module)`: frozen target + trainable predictor MLPs `[obs_dim] → [hidden] → [hidden] → [embed]`
- `RNDRewardWrapper(gym.Wrapper)`: wraps evader env, augments reward with `rnd_coef * normalized_intrinsic`
  - Handles both flat Box and Dict observation spaces via `_flatten_obs()`
  - Periodic predictor training every `update_freq` steps
  - Logs `rnd_intrinsic`, `rnd_extrinsic`, `rnd_total` in info dict
- Threaded through `_make_vec_env()` and `_train_phase()` — lazy module creation on first evader phase

### AMSDRL Integration
- Added `ewc_lambda`, `ewc_fisher_samples`, `rnd_coef`, `rnd_embed_dim`, `rnd_hidden_dim`, `rnd_update_freq` params
- EWC: snapshot on curriculum advancement, hooks around evader's `model.learn()`
- RND: wraps evader envs via `_make_vec_env()`, lazy init to auto-detect obs_dim
- Both disabled by default (lambda=0, coef=0) — zero runtime overhead when not used

### Tests (40 new tests)
- `tests/test_ewc.py`: 17 tests — init, snapshot, Fisher, penalty, hooks, training integration
- `tests/test_rnd.py`: 23 tests — RunningMeanStd, RNDModule, RNDRewardWrapper, Monitor compatibility

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `training/ewc.py` | Created | EWC regularizer with snapshot, hooks, penalty |
| `envs/rnd.py` | Created | RND module, reward wrapper, RunningMeanStd |
| `training/amsdrl.py` | Modified | EWC/RND integration: imports, __init__ params, _ewc_snapshot_evader(), _train_phase() hooks, _make_vec_env() RND wrapping |
| `scripts/train_amsdrl.py` | Modified | 6 new CLI args for EWC/RND |
| `tests/test_ewc.py` | Created | 17 EWC tests |
| `tests/test_rnd.py` | Created | 23 RND tests |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| EWC via gradient hooks (not PPO subclass) | Non-invasive integration with SB3; hooks inject penalty gradient during standard backprop |
| RND wraps after PartialObsWrapper | Flat concatenation of Dict obs ensures consistent input dim; operates on agent's actual observation |
| Lazy RND module creation | Auto-detects obs_dim on first evader training phase; handles Dict/Box spaces |
| Skip Fix 7 (diversity pool) | Already have `OpponentPool`; EWC+RND address the core forgetting/exploration issues |
| Dict obs flattening in RND | Sorts keys alphabetically for deterministic concatenation order |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| Dict obs space from PartialObsWrapper | Added `_flatten_obs()` to RNDRewardWrapper; flattened dim calc in `_train_phase()` |
| Tests needed `MultiInputPolicy` for Dict space | Used full_obs (flat Box) for EWC/RND unit tests; integration handled in AMSDRL |

## Next Steps
- [ ] Monitor Runs V and W for L2 collapse resolution
- [ ] If Tier 1+2 fixes resolve collapse, launch Runs with Tier 3 enabled
- [ ] Tune EWC lambda and RND coefficient via grid search
- [ ] Proceed to Phase 4 (sim-to-real) once self-play converges

## Metrics
- Files created: 4 (ewc.py, rnd.py, test_ewc.py, test_rnd.py)
- Files modified: 2 (amsdrl.py, train_amsdrl.py)
- Tests added: 40 (17 EWC + 23 RND)
- Total tests passing: 654 (614 core + 40 new, excluding 68 pre-existing Phase 2.5 BarrierNet failures)
