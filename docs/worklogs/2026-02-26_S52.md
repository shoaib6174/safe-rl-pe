# Session 52 — 2026-02-26

## Context
- **Phase**: Phase 3: Partial Observability & Self-Play
- **Branch**: main
- **Prereq**: S51 identified 8 root causes for evader L2 collapse, recommended 9 fixes in 3 tiers

## Objectives
- [x] Implement Tier 1 fixes: bilateral rollback, evader-first at new levels
- [x] Implement Tier 2 fixes: warm-start evader, mixed-level replay, smooth curriculum
- [x] Launch training Runs V+W (Tier 1+2) and X+Y (Tier 1+2+3) — 4 runs total
- [x] Implement Tier 3: EWC (Fix 5) and RND (Fix 9) — catastrophic forgetting prevention and exploration
- [x] Add tests for all new features (40 tests)
- [x] Update documentation

## Work Done

### Tier 1+2 Fixes (from first part of session)
- **Bilateral rollback**: `--bilateral_rollback` flag in AMSDRLSelfPlay
- **Evader-first on advance**: `--evader_first_on_advance` forces evader training at new curriculum levels
- **Warm-start evader**: `--warm_start_evader` with `--warm_start_timesteps` restores and pre-trains evader at transitions
- **Mixed-level replay**: `--mixed_level_ratio 0.2` keeps 20% of envs at previous level distances
- **Smooth curriculum**: `--smooth_curriculum` with `--smooth_curriculum_increment 0.5` for continuous difficulty scaling
- All Tier 1+2 CLI args added to `scripts/train_amsdrl.py`

### Training Runs V, W, X, Y Launched
- **Run V**: Equal speed (pursuer=1.0, evader=1.0), Tier 1+2 only, seed=47
- **Run W**: Pursuer 95% speed (pursuer=0.95, evader=1.0), Tier 1+2 only, seed=47
- **Run X**: Equal speed + Tier 3 (EWC λ=1000, RND coef=0.1), seed=47
- **Run Y**: Pursuer 95% + Tier 3 (EWC λ=1000, RND coef=0.1), seed=47
- All 4 running in background on local machine

### Tier 3: EWC (Elastic Weight Consolidation) — `training/ewc.py`
- `EWCRegularizer` class with:
  - `snapshot(model, obs_batch)`: saves theta_star, computes diagonal Fisher info via policy log-prob gradients
  - `register_hooks(model)`: injects `lambda * F * (theta - theta_star)` into gradients via PyTorch hooks
  - `remove_hooks(handles)`: cleans up after training
  - `penalty(model)`: scalar penalty for logging
- Integrates with SB3 PPO without subclassing — hooks inject EWC gradient during standard backprop
- Triggered on curriculum advancement in `AMSDRLSelfPlay.run()` via `_ewc_snapshot_evader()`

### Tier 3: RND (Random Network Distillation) — `envs/rnd.py`
- `RunningMeanStd`: Welford's online algorithm for reward normalization
- `RNDModule(nn.Module)`: frozen target + trainable predictor MLPs `[obs_dim] → [hidden] → [hidden] → [embed]`
- `RNDRewardWrapper(gym.Wrapper)`: wraps evader env, augments reward with `rnd_coef * normalized_intrinsic`
  - Handles both flat Box and Dict observation spaces via `_flatten_obs()`
  - Periodic predictor training every `update_freq` steps
  - Logs `rnd_intrinsic`, `rnd_extrinsic`, `rnd_total` in info dict
- Threaded through `_make_vec_env()` and `_train_phase()` — lazy module creation on first evader phase

### AMSDRL Integration
- Added `ewc_lambda`, `ewc_fisher_samples`, `rnd_coef`, `rnd_embed_dim`, `rnd_hidden_dim`, `rnd_update_freq` params
- EWC: snapshot on curriculum advancement, hooks around evader's `model.learn()`
- RND: wraps evader envs via `_make_vec_env()`, lazy init to auto-detect obs_dim
- Both disabled by default (lambda=0, coef=0) — zero runtime overhead when not used

### Tests (40 new tests)
- `tests/test_ewc.py`: 17 tests — init, snapshot, Fisher, penalty, hooks, training integration
- `tests/test_rnd.py`: 23 tests — RunningMeanStd, RNDModule, RNDRewardWrapper, Monitor compatibility

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `training/ewc.py` | Created | EWC regularizer with snapshot, hooks, penalty |
| `envs/rnd.py` | Created | RND module, reward wrapper, RunningMeanStd |
| `training/amsdrl.py` | Modified | EWC/RND integration: imports, __init__ params, _ewc_snapshot_evader(), _train_phase() hooks, _make_vec_env() RND wrapping |
| `scripts/train_amsdrl.py` | Modified | 6 new CLI args for EWC/RND |
| `tests/test_ewc.py` | Created | 17 EWC tests |
| `tests/test_rnd.py` | Created | 23 RND tests |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| EWC via gradient hooks (not PPO subclass) | Non-invasive integration with SB3; hooks inject penalty gradient during standard backprop |
| RND wraps after PartialObsWrapper | Flat concatenation of Dict obs ensures consistent input dim; operates on agent's actual observation |
| Lazy RND module creation | Auto-detects obs_dim on first evader training phase; handles Dict/Box spaces |
| Skip Fix 7 (diversity pool) | Already have `OpponentPool`; EWC+RND address the core forgetting/exploration issues |
| Dict obs flattening in RND | Sorts keys alphabetically for deterministic concatenation order |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| Dict obs space from PartialObsWrapper | Added `_flatten_obs()` to RNDRewardWrapper; flattened dim calc in `_train_phase()` |
| Tests needed `MultiInputPolicy` for Dict space | Used full_obs (flat Box) for EWC/RND unit tests; integration handled in AMSDRL |
| Local training OOM'd (8 GB MacBook, 4 concurrent runs) | Moved all training to niro-2 (125 GB RAM). Updated CLAUDE.md: NEVER train locally |
| RTX 5090 (sm_120) incompatible with PyTorch 2.6 | Upgraded to PyTorch 2.9.1+cu128 on niro-2; GPU fully utilized at 93% |
| Python stdout buffering hides nohup logs | Use `PYTHONUNBUFFERED=1`; also `tr '\r' '\n'` to see Rich progress bars in logs |
| Mixed-level replay crash with SmoothCurriculumManager | `SmoothCurriculumManager` has no `levels` list; fixed to use `max_init_distance - distance_increment` for previous level |
| EWC snapshot crash with Dict observations | Partial-obs environments return Dict obs; added flattening (sorted key concat) before `torch.FloatTensor` conversion |
| First run batch (V/W) crashed at S4-S6 Level 2, (X/Y) at S5 curriculum advance | Both bugs fixed in `amsdrl.py`; all 4 relaunched ~04:57 |

## Experimental Results

### First Batch (crashed due to bugs, ~03:55–09:30)

Both bugs triggered when Tier 1+2/3 features activated for the first time. Data lost (dirs cleaned for relaunch).

| Run | Reached | Last Metrics | Crash Cause |
|-----|---------|-------------|-------------|
| V | S6, Level 2 | SR_P=0.750, SR_E=0.250, NE gap=0.500 | `SmoothCurriculumManager.levels` (mixed-level replay bug) |
| W | S4, Level 2 | SR_P=0.730, SR_E=0.270, NE gap=0.460 | Same `levels` bug |
| X | S5, curriculum 5.0→5.5 | — | EWC snapshot Dict obs → tensor TypeError |
| Y | S5, curriculum 5.0→5.5 | — | Same EWC bug |

**Finding**: Mixed-level replay never actually ran in the first batch (crashed on first invocation), so Tier 1+2 was incomplete.

### Second Batch: Baselines V/X (bugfix relaunch, ~04:57, no structural fixes)

**Run V** (equal speed, no T3) — L1 oscillation, stuck at Level 1 through S7:

| Phase | Role | SR_P | SR_E | NE Gap | Rollbacks | Notes |
|-------|------|------|------|--------|-----------|-------|
| S1 | pursuer | 0.480 | 0.520 | 0.040 | 0 | Healthy balance |
| S2 | evader | 0.580 | 0.420 | 0.160 | 0 | Pursuer pulling ahead |
| S3 | pursuer | **1.000** | **0.000** | **1.000** | **3** | Total collapse — bilateral rollback fired 3x |
| S4 | evader | 0.260 | 0.740 | 0.480 | 1 | Evader recovers |
| S5 | pursuer | 0.350 | 0.650 | 0.300 | 0 | |
| S6 | evader | 0.030 | 0.970 | 0.940 | 0 | Evader over-corrects |
| S7 | pursuer | ... | ... | ... | ... | Training |

**Finding**: Classic L1 oscillation — wild swings (48% → 100% → 26% → 3%), never converging. Capture-rate threshold (0.70) unreachable with balanced play. Stuck at Level 1.

**Run X** (equal speed, +T3) — S3 rollback, same pattern emerging:

| Phase | Role | SR_P | SR_E | NE Gap | Rollbacks | Notes |
|-------|------|------|------|--------|-----------|-------|
| S1 | pursuer | 0.380 | 0.620 | 0.240 | 0 | OK |
| S2 | evader | 0.360 | 0.640 | 0.280 | 0 | Stable |
| S3 | pursuer | ... | ... | ... | 1+ | Rollback fired, still training |

**Runs W and Y** killed earlier (95% speed, unstable across seeds).

### Third Batch: Fix Runs Z1/Z2 (launched ~14:53, all 3 structural fixes)

Phase warmup active (100K steps S1-S2), NE-gap advancement, cold-start checkpoint saved.

**Run Z1** (fixes, no T3):

| Phase | Role | SR_P | SR_E | NE Gap | Steps | Notes |
|-------|------|------|------|--------|-------|-------|
| S1 | pursuer | **0.200** | 0.800 | 0.600 | 100K | Pursuer improves but doesn't dominate |
| S2 | evader | ... | ... | ... | 100K | Training |

**Run Z2** (fixes, +T3):

| Phase | Role | SR_P | SR_E | NE Gap | Steps | Notes |
|-------|------|------|------|--------|-------|-------|
| S1 | pursuer | **0.350** | 0.650 | 0.300 | 100K | Moderate pursuer, no domination |
| S2 | evader | ... | ... | ... | 100K | Training (RND active) |

**Early comparison** (S1 capture rate — lower is better for preventing L1 oscillation):
- V (500K, no fixes): 0.480 → collapsed to 1.000 by S3
- X (500K, +T3): 0.380 → rollback at S3
- **Z1 (100K, fixes): 0.200** — pursuer learning but not dominating
- **Z2 (100K, fixes+T3): 0.350** — similar, RND active

Phase warmup limits S1 pursuer improvement, preventing the first-mover domination that caused L1 oscillation in V. Z1/Z2 also cycle 5x faster per phase (~5 min vs ~30 min).

### Seed Sensitivity Experiment (3 runs, 95% speed, no Tier 3, seeds 1/2/3)

To test whether the W vs Y difference was seed sensitivity, launched 3 short runs (max 3 phases) with the W config but different seeds.

| Seed | Cold-Start Escape | S1 Capture | S1 Escape | NE Gap | Rollbacks |
|------|-------------------|-----------|-----------|--------|-----------|
| 1 | 0.83 | 0.50 | 0.50 | 0.000 | 5 |
| 2 | 0.72 | 0.96 | 0.04 | 0.920 | 3 |
| 3 | 0.94 | 1.00 | 0.00 | 1.000 | 14 |
| 47 (Run W) | 0.94 | 0.13 | 0.87 | 0.740 | 0 |
| 47 (Run Y, +T3) | 0.94 | 1.00 | 0.00 | 1.000 | 5 |

**Key findings**:
1. **Massive seed variance**: Same config produces capture rates from 0.13 to 1.00
2. **Run W (seed 47) is the outlier**: 3/4 seeds show pursuer domination. W's 13% capture was anomalous luck.
3. **The 95% speed pursuer is NOT too slow**: Seeds 2 and 3 achieve 96-100% capture even at 0.95 speed
4. **Bilateral rollback is broken in S1**: Always shows "0 rolling checkpoints available" for evader — no evader checkpoints saved after cold-start

### Historical Runs (from earlier sessions)

**Run H (curriculum, discrete levels)**:
- Reached L4 (full scenario) but evader collapsed: escape_rate → 0.0 at S8-S12
- Classic L2→L3→L4 collapse pattern
- 12 phases, did not converge

**Run I (opponent pool)**:
- "Converged" at L2 (NE gap < 0.1) after 3 phases
- Premature convergence — both agents balanced but at low skill level

## Root Cause Analysis (updated)

The S51 analysis identified L2 collapse (evader forgetting at curriculum transitions). The S52 experiments reveal a **deeper structural problem: first-mover advantage in self-play**.

### The Asymmetry
1. Cold-start evader trains against a **random** pursuer → learns trivial evasion
2. S1 pursuer gets **500K steps** against this weak frozen evader → easy to dominate
3. S2 evader gets 500K steps against a **fully-trained** pursuer → too hard to recover
4. Bilateral rollback can't fire in S1 because **no evader checkpoints exist** from cold-start

### Why Rollback Fails
- The health monitor detects capture domination (≥0.98) and rolls back the pursuer
- But the pursuer re-learns domination within ~20K steps (the problem is too easy)
- Bilateral rollback is supposed to also strengthen the evader, but skips because "0 rolling checkpoints available"
- No evader rolling checkpoints are saved after cold-start — a gap in the implementation

### Why Equal Speed Works Better
- At equal speed (V, X): pursuer capture ~38-48% at S1 → healthy balance
- At 95% speed (W, Y, seeds 1-3): capture ranges 13-100% → chaotic, seed-dependent
- Equal speed makes the pursuit problem harder, preventing the pursuer from trivially dominating a weak evader

## Fixes Implemented (later in session)

### Fix 1: Save evader checkpoint after cold-start (bug fix) — DONE
The bilateral rollback gap: no evader rolling checkpoints exist when S1 starts. Added `save_rolling()` and `save_milestone()` calls at the end of `_cold_start()` so bilateral rollback has evader checkpoints from the start.

### Fix 2: Phase length warmup (structural fix) — DONE
`--phase_warmup` flag with default schedule:
- S1-S2: 100K steps/phase
- S3-S4: 200K steps/phase
- S5-S6: 300K steps/phase
- S7+: full `timesteps_per_phase` (500K)

Prevents the pursuer from fully dominating in S1. With only 100K steps, it improves but can't reach 100% capture.

### Fix 3: NE-gap-based curriculum advancement — DONE
`--ne_gap_advancement` replaces the capture-rate threshold with balanced-play detection:
- Advance when `|SR_P - SR_E| < ne_gap_threshold` for `ne_gap_consecutive` consecutive phases
- Default: gap < 0.15 for 2 consecutive phases
- Uses `SmoothCurriculumManager._advance()` (new method) for distance increment
- Prevents the paradox where balanced play (the goal) blocks curriculum progression

### Files Changed by Fixes
| File | Changes |
|------|---------|
| `training/amsdrl.py` | Fix 1: cold-start checkpoint save, Fix 2: phase warmup logic in `__init__` + `_train_phase()`, Fix 3: NE-gap advancement in `run()`, header prints |
| `training/curriculum.py` | New `_advance()` method on `SmoothCurriculumManager`, refactored `check_advancement()` |
| `scripts/train_amsdrl.py` | 4 new CLI args: `--phase_warmup`, `--ne_gap_advancement`, `--ne_gap_threshold`, `--ne_gap_consecutive` |
| `tests/test_s52_fixes.py` | 17 tests covering all 3 fixes |

### Training Runs Z1, Z2 Launched
- **Run Z1**: Equal speed, all fixes, no Tier 3
- **Run Z2**: Equal speed, all fixes, + Tier 3 (EWC λ=1000, RND=0.1)
- Both running on niro-2 alongside V and X (baselines without fixes)

## Next Steps
- [x] Monitor Z1/Z2 vs V/X — S1 results confirm phase warmup prevents L1 domination
- [ ] Monitor Z1/Z2 through S3-S6 — key test is whether L1 oscillation is eliminated
- [ ] Check NE-gap advancement triggers — Z1/Z2 should advance when NE gap < 0.15 for 2 phases
- [ ] Compare V (oscillating wildly) vs Z1 (warmup) at comparable wall-clock time
- [ ] If Z1/Z2 advance past L1: monitor L2+ behavior for evader collapse
- [ ] Proceed to Phase 4 (sim-to-real) once self-play converges

## Active Training Runs on niro-2

### Baseline Runs (no fixes, still running as controls)

| Run | Dir | Speed | Fixes | Tier 3 | Status |
|-----|-----|-------|-------|--------|--------|
| V | `results/run_V_equal_speed` | 1.0/1.0 | None | No | S7+, L1 oscillation (48%→100%→26%→3%), stuck at Level 1 |
| X | `results/run_X_equal_speed_tier3` | 1.0/1.0 | None | EWC+RND | S3, rollback fired |

### Fix Runs (launched ~15:00, all 3 fixes active)

Both use: phase warmup (100K→200K→300K→500K), NE-gap advancement (gap<0.15, 2 consecutive),
cold-start checkpoint save, plus all Tier 1+2 (bilateral rollback, evader-first, warm-start,
mixed-level replay, smooth curriculum).

| Run | Dir | Speed | Tier 3 | Purpose |
|-----|-----|-------|--------|---------|
| Z1 | `results/run_Z1_fixes_noT3` | 1.0/1.0 | No | Fixes only |
| Z2 | `results/run_Z2_fixes_T3` | 1.0/1.0 | EWC+RND | Fixes + Tier 3 |

### Killed Runs
- W (95% speed, no T3) — pursuer couldn't learn, stuck at 13% capture
- Y (95% speed, +T3) — collapsed at L1, 100% capture, unrecoverable
- Seed tests 1, 2, 3 — completed (showed massive seed variance)

**How to check progress:**
```bash
sshpass -p '123456' ssh niro-2@100.71.2.97 "cat ~/claude_pursuit_evasion/results/run_Z1_fixes_noT3/train.log | tr '\r' '\n' | grep -E '(SR_|Phase|CURRICULUM|ROLLBACK|WARMUP|NE-GAP)'"
```

## Metrics
- Files created: 5 (ewc.py, rnd.py, test_ewc.py, test_rnd.py, test_s52_fixes.py)
- Files modified: 3 (amsdrl.py, train_amsdrl.py, curriculum.py)
- Tests added: 57 (17 EWC + 23 RND + 17 S52 fixes)
- Total tests passing: 687 (excluding pre-existing BarrierNet/QP failures)
- Training runs: 4 active on niro-2 (2 baselines V/X + 2 fix runs Z1/Z2)
- Bugs found and fixed: 2 (mixed-level replay + EWC Dict obs)
- Fixes implemented: 3 (cold-start checkpoint, phase warmup, NE-gap advancement)
