# Session 59 — 2026-03-01

## Context
- **Phase**: Phase 3 — Partial Observability & Self-Play
- **Continuation**: From S58 extended session (warm-seed implementation, SP1 analysis, variable speed decision)

## Objectives
- [x] Implement variable speed support in diagnostic training scripts
- [x] Update visualization script for auto-detecting action dimensions
- [x] Run tests — no regressions
- [x] Push and launch S1v2 (evader vs greedy, variable speed) on niro-2
- [x] S1v2 complete: 12% escape at 2M (not converged)
- [x] Launch S1v2b (5M steps, ent_coef=0.03) — 98% escape, obstacle-hugging perfected
- [x] Launch SP2: cold-start variable-speed self-play — **KILLED** (pursuer stuck 0-7% at 2.87M)
- [x] Launch 4 parallel self-play experiments (Run A/B/C/D)

## Work Done

### Variable Speed Support (Removing FixedSpeedWrapper)
**Rationale**: SP1 self-play showed classic cycling — evader only has one viable strategy (obstacle-hugging) at fixed speed, so once the pursuer cracks it, there's no fallback. Variable speed (2D [v, omega] actions) gives the evader speed-turn tradeoff, sprint-and-orbit, and feinting capabilities.

**Key insight**: `training/amsdrl.py` already supports `fixed_speed=False` throughout — the change is only needed in diagnostic scripts and visualization.

### Modified `scripts/train_evader_vs_greedy.py`
- Added `--fixed_speed` flag (default False = variable speed)
- `make_env()` and `evaluate()` conditionally apply FixedSpeedWrapper
- Banner shows action mode

### Modified `scripts/train_pursuer_vs_evader.py`
- Added `--fixed_speed` flag (default False)
- Updated `PPOOpponentPolicy` to auto-detect action dimension:
  - 1D model (fixed speed): [omega] → [v_max, omega]
  - 2D model (variable speed): use directly
- `make_env()` and `evaluate()` conditionally apply FixedSpeedWrapper
- Banner shows action mode and opponent model dimension

### Modified `scripts/visualize_both_learned_gif.py`
- Auto-detects pursuer/evader model action dimensions
- Handles both 1D and 2D models seamlessly

### S1v2 and S1v2b Results (Evader vs Greedy, Variable Speed)
- **S1v2** (2M steps, ent_coef=0.01): Peak 12% escape at 2M — too slow learning with 2D action space
- **S1v2b** (5M steps, ent_coef=0.03): Escape rate exploded 5% → 96% at 1.8M steps, sustained 94-100% through completion. Final: **98% escape rate**
- **Key finding**: Variable speed made obstacle-hugging MORE effective (37%→98%) but did NOT create new strategies. Same single behavior, just better executed (tighter orbits via v/omega tradeoff)
- **Decision**: Skip further diagnostics, go straight to self-play

### SP2: Cold-Start Variable-Speed Self-Play — FAILED
- **Config**: 10x10, 2 obs, equal speed, ±50 terminal, 200K cold-start
- **Result**: Killed at 2.87M/10M steps. Pursuer stuck at 0-7% capture rate (max 7% at M900)
- **Root cause — bootstrapping failure**: 200K cold-start created a near-perfect evader (98% vs greedy). Pursuer starting from scratch could never bridge the skill gap. Adaptive boost gave pursuer all training time but with 0-7% capture rate there was insufficient positive reward signal to learn from.
- **Key data point**: Greedy pursuer only manages 3% capture vs S1v2b — confirms the variable-speed obstacle-hugging evader is nearly unbeatable at equal speed.

### Parallel Experiment Sweep (4 runs on niro-2)

| Run | Name | Config | Hypothesis |
|-----|------|--------|------------|
| **A** | P_vs_S1v2b_varspeed | Pursuer training vs S1v2b, 3M steps | Focused training gives pursuer a chance; warm-seed both into self-play after |
| **B** | SP2b_shortcold | Equal speed, 50K cold-start, 10M | Weaker evader head-start reduces bootstrapping gap |
| **C** | SP2c_105x_nocold | 1.05x pursuer speed, 0 cold-start, 10M | Speed advantage + co-evolution — game is physically solvable for pursuer |
| **D** | SP2d_equal_nocold | Equal speed, 0 cold-start, 10M | Pure co-evolution baseline — isolates cold-start vs speed effects |

All share: variable speed, 10x10, 2 obs, ±50 terminal, ent_coef=0.03, pool 50, adaptive_ratio=0.3, lr_dampen=0.3, collapse_threshold=0.05, PFSP, seed 47.

**Results (40 min):**
- **A**: 1.8M/3M, **pursuer exploded** 8%→21%→45%→67%→94%. Saturated 86-94% since 1.15M. Clear winner.
- **B**: 1.02M/10M, max 3%, regressed to 0%. **Dead — killed.**
- **C**: 819K/10M, max 5%, regressed to 0%. **Dead — killed.**
- **D**: 410K/10M, max 3%, 2% latest. **Dead — killed.**

**Key insight**: Cold-start self-play fundamentally cannot bootstrap the pursuer with variable speed. The pursuer needs focused training against a fixed opponent first, then warm-seed into self-play.

### SP3: Warm-Seeded Variable-Speed Self-Play — REAL DYNAMICS
- **Init**: `pursuer_best.zip` (95% capture from Run A) + `evader_final.zip` (98% escape from S1v2b)
- **S0 warm-seed eval: SR_P=0.49, SR_E=0.51** — near-perfect balance from the start
- **Config**: same as SP2 (10x10, 2 obs, ±50, pool 50, all anti-cycling), 10M steps
- **PID**: 2007289 on niro-2

**SP3 progress (3.38M/10M, 47 min):**
| Phase | SR_P | SR_E | Gap | Pattern |
|-------|------|------|-----|---------|
| S0 | 0.49 | 0.51 | 0.02 | Warm-seed balance |
| M50-M500 | 0-15% | 85-100% | ~0.9 | Evader dominated after first training |
| M550-M600 | 65-88% | 12-35% | ~0.7 | Pursuer breakthrough #1 |
| M650-M950 | 7-29% | 71-93% | ~0.5-0.8 | Cycling, both adapting |
| **M1100** | **49%** | **51%** | **0.02** | **Near-convergence** |
| M1150-M1350 | 42-88% | 12-58% | 0.16-0.76 | Pursuer surging with balance points |
| M1450-M1650 | 73-98% | 2-27% | 0.46-0.96 | Pursuer dominating, evader training |

- **Min gap: 0.02** at M1100 — tantalizingly close to convergence (eta=0.10)
- Both agents have reached 80%+ at different times — real arms race
- Classic cycling but with genuine competition, unlike all prior self-play runs
- Currently in pursuer-dominated phase; evader collapse rollback may trigger

**Visualizations generated:**
- `results/sp3_grid_M600_peak.gif` — Pursuer peak: 9/9 captures
- `results/sp3_grid_M990.gif` — Evader recovery: 0/9 captures

### SP3 Extended Progress (M1800, 3.69M/10M)
- Pursuer dominated M1200-M1800 (73-98% capture) despite evader getting all training
- Evader had 21 consecutive training phases (700K+ steps) but couldn't recover
- **Root cause**: Evader only knows obstacle-hugging; once cracked, no fallback strategy
- Near-collapse territory (SR_E=0.02 at M1500) but evader recovers just enough to avoid rollback

### Phase 3: LOS-Based Partial Observability
**Rationale**: SP3 confirmed the evader's single-strategy problem. Deep analysis identified 5 approaches (see `docs/evader_strategy_analysis.md`). Partial observability is the highest-impact change because it creates an information management game — hiding behind obstacles denies the opponent knowledge of your position.

**Implementation**:
- `ObservationBuilder.build()`: accepts `los_blocked` flag; when True + `partial_obs=True`, zeros opponent features (position, heading, velocity, distance, bearing)
- Added `los_visible` binary flag to observation (1.0 = can see opponent, 0.0 = occluded)
- Obs dim: 15 + 2*K (vs 14 + 2*K for full obs) — +1 for los_visible flag
- `PursuitEvasionEnv._get_obs()`: computes LOS using existing `line_of_sight_blocked()`
- `--partial_obs_los` CLI flag for self-play, `--partial_obs` for diagnostic scripts
- Uses existing infrastructure: `line_of_sight_blocked()` already implemented and tested (12 tests)

**S1v3 launched**: Evader vs greedy, partial obs, variable speed, 5M steps, ent_coef=0.03
- PID 2066002 on niro-2
- Same config as S1v2b but with `--partial_obs`

### Tests
- All 129 core tests pass (37 opponent_pool + 51 env + 41 rewards)
- 9 new tests for partial observability (TestPartialObsLOS)
- No regressions

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `scripts/train_evader_vs_greedy.py` | Modified | Added `--fixed_speed` flag, `--partial_obs` flag |
| `scripts/train_pursuer_vs_evader.py` | Modified | Added `--fixed_speed` flag, auto-detect PPOOpponentPolicy |
| `scripts/train_amsdrl.py` | Modified | Added `--partial_obs_los` flag |
| `scripts/visualize_both_learned_gif.py` | Modified | Auto-detect action dim for both 1D and 2D models |
| `envs/observations.py` | Modified | Added `partial_obs` flag, LOS masking, `los_visible` flag (+1 dim) |
| `envs/pursuit_evasion_env.py` | Modified | Added `partial_obs` param, LOS computation in `_get_obs()` |
| `training/amsdrl.py` | Modified | Added `partial_obs_los` param, flow-through to env_kwargs |
| `tests/test_env.py` | Modified | Added 9 TestPartialObsLOS tests |
| `docs/evader_strategy_analysis.md` | Created | Strategy analysis: 5 approaches to break obstacle-hugging |
| `docs/worklogs/2026-03-01_S59.md` | Created | This worklog |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Variable speed over 3 obstacles or RND | Speed-turn tradeoff fundamentally expands evader strategy space; obstacles add clutter risk; RND only helps exploration, not strategy |
| Default `fixed_speed=False` | Variable speed is the new default; old 1D behavior preserved via `--fixed_speed` flag for reproducibility |
| Auto-detect action dim in PPOOpponentPolicy | Allows mixing 1D and 2D models (e.g., 1D evader opponent with 2D pursuer) |
| ent_coef=0.03 for variable speed | S1v2 (0.01) stalled at 12%; S1v2b (0.03) exploded to 98% — higher entropy critical for 2D action exploration |
| Kill SP2, launch 4-run sweep | SP2's bootstrapping failure showed 200K cold-start is too much. Sweep tests 4 hypotheses in parallel |
| Kill B/C/D, keep A | All 3 self-play cold-starts failed identically (0-5% pursuer). Focused training (A) is the only viable path. |
| Warm-seeded self-play (SP3) | Both agents start competent (49/51 balance). First self-play with real arms-race potential. |
| LOS partial obs over wrapper-based partial obs | Lightweight (flat obs, MlpPolicy), integrates into existing self-play pipeline. Wrapper-based system (FOV+lidar+LSTM) too complex for initial test. |
| Symmetric LOS masking | Both agents lose opponent info when occluded. Simpler and more principled than asymmetric. |

## Next Steps
- [ ] Monitor S1v3 — does partial obs create new strategies beyond obstacle-hugging?
- [ ] Monitor SP3 — still running, check if evader eventually recovers
- [ ] If S1v3 shows diverse strategies: train pursuer vs S1v3 evader, then warm-seed into SP4
- [ ] If S1v3 still only obstacle-hugs: combine with randomized obstacle count (Approach 2)

## Metrics
- Files changed: 10
- Tests: 129 pass, 0 regressions
- Training runs launched: 9 (S1v2, S1v2b, SP2 killed, A, B killed, C killed, D killed, SP3, S1v3)
