# Session 59 — 2026-03-01

## Context
- **Phase**: Phase 3 — Partial Observability & Self-Play
- **Continuation**: From S58 extended session (warm-seed implementation, SP1 analysis, variable speed decision)

## Objectives
- [x] Implement variable speed support in diagnostic training scripts
- [x] Update visualization script for auto-detecting action dimensions
- [x] Run tests — no regressions
- [ ] Kill SP1 on niro-2 (pursuer-dominated, wasting compute)
- [ ] Push and launch S1v2 (evader vs greedy, variable speed) on niro-2

## Work Done

### Variable Speed Support (Removing FixedSpeedWrapper)
**Rationale**: SP1 self-play showed classic cycling — evader only has one viable strategy (obstacle-hugging) at fixed speed, so once the pursuer cracks it, there's no fallback. Variable speed (2D [v, omega] actions) gives the evader speed-turn tradeoff, sprint-and-orbit, and feinting capabilities.

**Key insight**: `training/amsdrl.py` already supports `fixed_speed=False` throughout — the change is only needed in diagnostic scripts and visualization.

### Modified `scripts/train_evader_vs_greedy.py`
- Added `--fixed_speed` flag (default False = variable speed)
- `make_env()` and `evaluate()` conditionally apply FixedSpeedWrapper
- Banner shows action mode

### Modified `scripts/train_pursuer_vs_evader.py`
- Added `--fixed_speed` flag (default False)
- Updated `PPOOpponentPolicy` to auto-detect action dimension:
  - 1D model (fixed speed): [omega] → [v_max, omega]
  - 2D model (variable speed): use directly
- `make_env()` and `evaluate()` conditionally apply FixedSpeedWrapper
- Banner shows action mode and opponent model dimension

### Modified `scripts/visualize_both_learned_gif.py`
- Auto-detects pursuer/evader model action dimensions
- Handles both 1D and 2D models seamlessly

### Tests
- All 120 core tests pass (37 opponent_pool + 42 env + 41 rewards)
- No regressions

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `scripts/train_evader_vs_greedy.py` | Modified | Added `--fixed_speed` flag, conditional FixedSpeedWrapper |
| `scripts/train_pursuer_vs_evader.py` | Modified | Added `--fixed_speed` flag, auto-detect PPOOpponentPolicy |
| `scripts/visualize_both_learned_gif.py` | Modified | Auto-detect action dim for both 1D and 2D models |
| `docs/worklogs/2026-03-01_S59.md` | Created | This worklog |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Variable speed over 3 obstacles or RND | Speed-turn tradeoff fundamentally expands evader strategy space; obstacles add clutter risk; RND only helps exploration, not strategy |
| Default `fixed_speed=False` | Variable speed is the new default; old 1D behavior preserved via `--fixed_speed` flag for reproducibility |
| Auto-detect action dim in PPOOpponentPolicy | Allows mixing 1D and 2D models (e.g., 1D evader opponent with 2D pursuer) |

## Next Steps
- [ ] Kill SP1 on niro-2 (PID 1822323)
- [ ] Push code to GitHub
- [ ] Launch S1v2: `train_evader_vs_greedy.py` with variable speed (equal speed, 2 obstacles, D3 config)
- [ ] After S1v2 completes: launch diagnostic pursuer training against S1v2 evader
- [ ] After both: warm-seeded self-play with variable speed

## Metrics
- Files changed: 3
- Tests: 120 pass, 0 regressions
