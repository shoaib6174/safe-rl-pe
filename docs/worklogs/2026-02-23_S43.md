# Session 43 — 2026-02-23

## Context
- **Phase**: Phase 3: Partial Observability & Belief-Space Planning
- **Focus**: Opponent Pool for Self-Play Diversity

## Objectives
- [x] Create `training/opponent_pool.py` — OpponentPool class
- [x] Modify `training/amsdrl.py` — Pool integration in `_train_phase`
- [x] Add `--opponent_pool_size` CLI arg to `scripts/train_amsdrl.py`
- [x] Create `tests/test_opponent_pool.py` — Unit and integration tests
- [ ] Run tests and verify
- [ ] Launch Run I on niro-2

## Work Done

### OpponentPool Class (`training/opponent_pool.py`)
- Stores checkpoint directory paths (not models) for memory efficiency
- Lazy-loads and caches PPO models via `get_model()`
- `include_random=True` adds None sentinel to candidate pool (random policy)
- FIFO eviction when pool exceeds `max_size`
- Duplicate checkpoint paths are ignored
- `sample(n)` returns n opponents for per-sub-env assignment

### AMSDRLSelfPlay Integration (`training/amsdrl.py`)
- Added `opponent_pool_size` param to `__init__` (default 0 = disabled)
- When > 0, creates `pursuer_pool` and `evader_pool` OpponentPool instances
- Extracted `_wrap_opponent_model()` helper method to reduce code duplication
- Modified `_train_phase()`:
  - When pool enabled and has checkpoints, samples diverse opponents per sub-env
  - Each sub-env can get a different historical opponent or random policy
  - After milestone save, adds checkpoint to the trained role's pool
- Added pool status to `run()` print block

### CLI Integration (`scripts/train_amsdrl.py`)
- Added `--opponent_pool_size N` argument (default 0)
- Passed through to AMSDRLSelfPlay constructor

### Tests (`tests/test_opponent_pool.py`)
- 16 tests covering:
  - OpponentPool: empty pool sampling, add, duplicate rejection, FIFO eviction, cache eviction, sample with/without random, sample count, get_model load+cache, clear_cache, repr, len
  - AMSDRLSelfPlay integration: pool_size=0 (disabled), pool_size>0 (enabled), _wrap_opponent_model with None/model/fixed_speed

### Convergence Criterion Fix (`training/amsdrl.py`)
- Run I converged at Level 2 (NE gap=0.02) but never reached obstacles (Levels 3-4)
- Added `curriculum_at_max` gate: convergence only declared when curriculum is at max level
- Informational message printed when NE gap < eta but curriculum not at max

### Visualization Scripts (uncommitted)
- `scripts/visualize_runs.py` — Multi-panel comparison of training runs (capture rates, NE gap, capture time)
- `scripts/visualize_trajectories.py` — Loads trained models, runs episodes, plots trajectories with obstacles

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `training/opponent_pool.py` | Created | OpponentPool class with add/sample/get_model/eviction |
| `training/amsdrl.py` | Modified | Added pool params, _wrap_opponent_model helper, pool sampling in _train_phase, curriculum convergence gate |
| `scripts/train_amsdrl.py` | Modified | Added --opponent_pool_size CLI arg |
| `tests/test_opponent_pool.py` | Created | 18 unit + integration tests |
| `scripts/visualize_runs.py` | Created | Training run comparison plots |
| `scripts/visualize_trajectories.py` | Created | Trajectory visualization with obstacle rendering |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Store paths, not models | Memory efficiency — lazy load + cache on demand |
| Include random policy in pool | Prevents forgetting basic skills; random opponent is strong baseline |
| Per-sub-env sampling | Maximum diversity — each parallel env trains against different opponent |
| FIFO eviction | Simple, deterministic; oldest opponents are least relevant |
| Extract _wrap_opponent_model | Reduces duplication between pool and non-pool code paths |
| Curriculum gate for convergence | Run I showed NE gap can reach 0.02 at Level 2 without obstacle testing — must require max curriculum level |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| BiMDN adapter very slow on CPU for trajectory viz | Use `--device cuda` on niro-2; killed stuck 3-ep process after 111 min |
| Obstacle dict key `radius` not `r` | Fixed in visualize_trajectories.py |

## Run I Results (COMPLETED — CONVERGED)
- **Config**: Same as Run H + `--opponent_pool_size 5`
- **Duration**: 0.5h (1842s) — 3.4x faster than Run H (6261s)
- **Phases**: 4 (S0-S3) vs Run H's 12
- **Converged**: Yes, at S3 with NE gap = 0.020
- **Final**: SR_P=0.49, SR_E=0.51 — near-perfect Nash Equilibrium
- **Curriculum**: Only reached Level 2 (medium distance, no obstacles)

### Phase-by-Phase
| Phase | Role | SR_P | SR_E | NE Gap | Level | Pool Status |
|-------|------|------|------|--------|-------|-------------|
| S0 | evader | 0.33 | 0.67 | 0.34 | 1 | — (cold-start) |
| S1 | pursuer | 0.84 | 0.16 | 0.68 | 1→2 | No checkpoints yet |
| S2 | evader | 0.26 | 0.74 | 0.48 | 2 | 14 model, 2 random |
| S3 | pursuer | 0.49 | 0.51 | **0.02** | 2 | 7 model, 9 random |

### Run H vs Run I Comparison
| Metric | Run H (Curriculum) | Run I (Curriculum + Pool) |
|--------|-------------------|--------------------------|
| Converged | No | **Yes** |
| Phases | 12 | **4** |
| Time | 1.7h | **0.5h** |
| Final NE Gap | 1.00 (collapse) | **0.02** |
| Min NE Gap | 0.26 | **0.02** |
| Final SR_P/SR_E | 1.00/0.00 | **0.49/0.51** |
| Curriculum Level | 4 (full) | 2 (no obstacles) |

### Analysis
- **Opponent pool completely prevents Level 4 collapse** — the core problem is solved
- **BUT**: Premature convergence at Level 2 — agents never tested with obstacles
- **Root cause**: Convergence criterion (`NE gap < 0.10`) has no curriculum gate
- **Fix needed**: Require `curriculum_at_max=True` before allowing convergence declaration

## Run H Trajectory Analysis
- **Without obstacles**: 3/3 captured (12.2s, 8.7s, 8.7s) — evader has no evasion strategy
- **With obstacles (n=3)**: 6/6 captured (5.1s, 6.5s, 1.8s, 6.0s, 7.2s, 7.1s) — evader never uses obstacles for cover
- Confirms Level 4 collapse: evader policy frozen too early, can't adapt

## Git Status
- 2 commits ahead of origin (not pushed):
  - `67a2b9f` — Session S43: Opponent pool
  - `d65615b` — Sessions S36-S42: Full Phase 3 implementation
- 2 uncommitted files: `scripts/visualize_runs.py`, `scripts/visualize_trajectories.py`

## Next Steps
- [x] Wait for Run I to complete — DONE, converged in 0.5h
- [x] Compare Run I vs Run H — opponent pool prevents collapse, NE gap 0.02 vs 1.00
- [x] Generate Run H vs Run I comparison plot — saved to `results/stage3/run_h_vs_i_comparison.png`
- [x] Fix premature convergence: added `curriculum_at_max` gate to convergence criterion in `amsdrl.py:636`
- [x] Launch Run J on niro-2 (PID 921598) with fixed convergence criterion
- [x] Run J showed same Level 4 collapse (100% pursuer at L4) — opponent pool alone insufficient
- [x] Root cause: L3→L4 jump too large (close+obs → variable-distance+obs). Distance-only reward gives no obstacle incentive
- [x] Fix A: Expanded curriculum from 4 to 6 levels (added L4: medium+obs, L5: far+obs)
- [x] Killed Run J, launched Run K (PID 998549) with 6-level curriculum, 18 max phases
- [ ] Wait for Run K to complete
- [ ] Compare Run K results
- [ ] Commit all changes

## Metrics
- Files changed: 6 (4 new, 2 modified) + 2 uncommitted viz scripts
- Tests added: 18 (all passing on niro-2)
- Documents updated: 2 (worklog + tracker)
