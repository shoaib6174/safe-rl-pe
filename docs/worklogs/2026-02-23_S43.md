# Session 43 — 2026-02-23

## Context
- **Phase**: Phase 3: Partial Observability & Belief-Space Planning
- **Focus**: Opponent Pool for Self-Play Diversity

## Objectives
- [x] Create `training/opponent_pool.py` — OpponentPool class
- [x] Modify `training/amsdrl.py` — Pool integration in `_train_phase`
- [x] Add `--opponent_pool_size` CLI arg to `scripts/train_amsdrl.py`
- [x] Create `tests/test_opponent_pool.py` — Unit and integration tests
- [ ] Run tests and verify
- [ ] Launch Run I on niro-2

## Work Done

### OpponentPool Class (`training/opponent_pool.py`)
- Stores checkpoint directory paths (not models) for memory efficiency
- Lazy-loads and caches PPO models via `get_model()`
- `include_random=True` adds None sentinel to candidate pool (random policy)
- FIFO eviction when pool exceeds `max_size`
- Duplicate checkpoint paths are ignored
- `sample(n)` returns n opponents for per-sub-env assignment

### AMSDRLSelfPlay Integration (`training/amsdrl.py`)
- Added `opponent_pool_size` param to `__init__` (default 0 = disabled)
- When > 0, creates `pursuer_pool` and `evader_pool` OpponentPool instances
- Extracted `_wrap_opponent_model()` helper method to reduce code duplication
- Modified `_train_phase()`:
  - When pool enabled and has checkpoints, samples diverse opponents per sub-env
  - Each sub-env can get a different historical opponent or random policy
  - After milestone save, adds checkpoint to the trained role's pool
- Added pool status to `run()` print block

### CLI Integration (`scripts/train_amsdrl.py`)
- Added `--opponent_pool_size N` argument (default 0)
- Passed through to AMSDRLSelfPlay constructor

### Tests (`tests/test_opponent_pool.py`)
- 16 tests covering:
  - OpponentPool: empty pool sampling, add, duplicate rejection, FIFO eviction, cache eviction, sample with/without random, sample count, get_model load+cache, clear_cache, repr, len
  - AMSDRLSelfPlay integration: pool_size=0 (disabled), pool_size>0 (enabled), _wrap_opponent_model with None/model/fixed_speed

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `training/opponent_pool.py` | Created | OpponentPool class with add/sample/get_model/eviction |
| `training/amsdrl.py` | Modified | Added pool params, _wrap_opponent_model helper, pool sampling in _train_phase |
| `scripts/train_amsdrl.py` | Modified | Added --opponent_pool_size CLI arg |
| `tests/test_opponent_pool.py` | Created | 16 unit + integration tests |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Store paths, not models | Memory efficiency — lazy load + cache on demand |
| Include random policy in pool | Prevents forgetting basic skills; random opponent is strong baseline |
| Per-sub-env sampling | Maximum diversity — each parallel env trains against different opponent |
| FIFO eviction | Simple, deterministic; oldest opponents are least relevant |
| Extract _wrap_opponent_model | Reduces duplication between pool and non-pool code paths |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| None yet | — |

## Next Steps
- [ ] Run tests: `./venv/bin/python -m pytest tests/test_opponent_pool.py tests/test_session3.py -v`
- [ ] Launch Run I on niro-2: `--opponent_pool_size 5 --curriculum`
- [ ] Compare oscillation behavior vs Run G/H

## Metrics
- Files changed: 4 (2 new, 2 modified)
- Tests added: 16
- Documents updated: 2 (worklog + tracker)
