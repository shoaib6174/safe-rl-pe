# Session 47 — 2026-02-24

## Context
- **Phase**: Phase 3 — Stage 3 self-play collapse analysis
- **Previous**: S45 implemented PBRS (w_obs_approach=10.0). S46 researched self-play collapse prevention (6 approaches, 20 papers). Run P completed 18 phases — **collapsed at Level 3+** (same as Runs H-O).

## Objectives
- [x] Deep analysis of Run P failure
- [x] Code-level analysis of curriculum advancement logic
- [x] Research-backed alternative approaches
- [x] Update documentation with findings and paper references

## Work Done

### 1. Run P Results Analysis

Run P completed all 18 phases in 2.3h on niro-2 (RTX 5090). **Did not converge.**

| Phase | Role | SR_P | SR_E | NE gap | Level | Notes |
|-------|------|------|------|--------|-------|-------|
| S0 | Cold-start | 0.41 | 0.59 | — | 1 | |
| S1 | Pursuer | 0.40 | 0.60 | 0.20 | 1 | Best NE gap |
| S2 | Evader | 0.18 | 0.82 | 0.64 | 1 | |
| S3 | Pursuer | 0.77 | 0.23 | 0.54 | 1→2 | |
| S4 | Evader | 0.80 | 0.20 | 0.60 | 2→3 | Evader trained at L2 (no obstacles) |
| S5 | Pursuer | 0.92 | 0.08 | 0.84 | 3→4 | L3 used for 1 pursuer phase only |
| S6 | Evader | 0.97 | 0.03 | 0.94 | 4→5 | First evader obstacle training |
| S7 | Pursuer | 0.99 | 0.01 | 0.98 | 5→6 | 1 rollback |
| S8 | Evader | 0.98 | 0.02 | 0.96 | 6 | |
| S9 | Pursuer | 1.00 | 0.00 | 1.00 | 6 | 4 rollbacks |
| S10 | Evader | 0.89 | 0.11 | 0.78 | 6 | Best L6 evader performance |
| S11 | Pursuer | 0.94 | 0.06 | 0.88 | 6 | 4 rollbacks |
| S12 | Evader | 0.93 | 0.07 | 0.86 | 6 | |
| S13 | Pursuer | 1.00 | 0.00 | 1.00 | 6 | 5 rollbacks |
| S14 | Evader | 0.97 | 0.03 | 0.94 | 6 | |
| S15 | Pursuer | 0.99 | 0.01 | 0.98 | 6 | 6 rollbacks |
| S16 | Evader | 1.00 | 0.00 | 1.00 | 6 | Worst evader phase |
| S17 | Pursuer | 1.00 | 0.00 | 1.00 | 6 | 5 rollbacks |
| S18 | Evader | 0.97 | 0.03 | 0.94 | 6 | |

**Final**: SR_P=0.97, SR_E=0.03, NE gap=0.94, converged=False. 25 total health rollbacks.

### 2. Root Cause Analysis (3 compounding failures)

**Root Cause 1: Curriculum races past obstacle levels**
- `curriculum.py:check_advancement()` checks ONLY pursuer capture_rate > 0.70
- Checked after EVERY phase (both pursuer AND evader), not just pursuer phases
- No minimum phases per level, no evader performance check
- Result: L1→L6 in 5 consecutive advancements (S3-S7)
- **The evader NEVER trained at L3** (close + obstacles) — L3 existed for only 1 pursuer phase

**Root Cause 2: Evader has zero obstacle experience when obstacles first appear**
- L1 and L2 have 0 obstacles → PBRS returns 0, visibility falls back to zero-sum
- Evader builds a "flee in straight lines" policy for 800k steps
- First obstacle exposure at L4 (S6), but pursuer already at 92% capture rate
- Starting from 92% deficit is essentially unrecoverable

**Root Cause 3: PBRS magnitude too small relative to other reward signals**
- PBRS per step: `10.0 * 0.1 / 14.14 ≈ 0.07`
- Visibility reward: ±1.0 per step
- Survival bonus: +1.0 per step
- PBRS gradient is **14x weaker** than visibility signal — drowned out

### 3. Code-Level Findings

**Curriculum advancement** (`training/curriculum.py`):
- `check_advancement()` at line ~143: `if capture_rate > self.advancement_threshold`
- Called from `amsdrl.py` line ~668-676 after EVERY phase regardless of role
- Increments by exactly 1 level per call, but can be called every 2 phases
- No minimum episodes per level, no evader metric check, no regression

**Health rollbacks** (`training/selfplay_callbacks.py`):
- Detect capture domination (≥0.98) and entropy collapse
- Roll back to 3 checkpoints earlier within a phase
- **Do NOT interact with curriculum** — rollback happens but curriculum still advances

**Level definitions** (`training/curriculum.py`):
- L1: 2-5m, 0 obstacles | L2: 5-15m, 0 obstacles
- L3: 2-5m, 3 obstacles | L4: 5-8m, 3 obstacles
- L5: 5-12m, 3 obstacles | L6: 2-15m, 3 obstacles (full scenario)

### 4. PBRS Effectiveness Assessment

- PBRS was active for 2.1M of 2.9M evader steps (72%) — BUT only after the pursuer was already dominant
- No PBRS-specific metrics logged (no reward decomposition, no r_pbrs tracking)
- Evader sigma increased 1.03→1.30 throughout training (never found coherent strategy, kept adding exploration noise)
- Episode lengths declined at L6 (384→377 steps), indicating pursuer catching evader faster

### 5. Research-Backed Fix (3-pronged)

| Priority | Fix | Mechanism | Reference |
|----------|-----|-----------|-----------|
| 1 | **Dual-criteria curriculum gate** | Advance only when BOTH capture_rate > 0.70 AND escape_rate > 0.10, minimum 4 phases per level, allow regression | Dennis et al. (PAIRED, NeurIPS 2020), Narvekar et al. (JMLR 2020) |
| 2 | **Asymmetric training ratio** | Evader gets 2-3x training steps at obstacle levels | Bansal et al. (ICLR 2018), Rao et al. (MASP, 2024) |
| 3 | **Increase PBRS weight** | w_obs_approach: 10→50 (0.35/step vs ±1.0 visibility) | Ng et al. (ICML 1999), Devlin & Kudenko (2012) |

**Deferred (if above insufficient):**
- NFSP (Heinrich & Silver 2016) — prevents strategy forgetting
- PSRO (Lanctot et al. 2017) — population + Nash meta-solver
- PAIRED (Dennis et al. 2020) — regret-based environment design
- Exploitability descent (Lockhart et al. 2019) — direct NE gap optimization

## Files Changed

| File | Action | Description |
|------|--------|-------------|
| `docs/worklogs/2026-02-24_S47.md` | Created | This worklog |
| `docs/worklogs/2026-02-24_S45.md` | Updated | Added Run P results and bug fix documentation |
| `docs/run_p_analysis.md` | Created | Detailed Run P training dynamics report |
| `docs/references/bibtex_references.bib` | Updated | Added 20 self-play/curriculum references |
| `docs/research_self_play_collapse_prevention.md` | Existing | Full research report from S46 (20 papers, 6 approaches) |
| `docs/workflow_tracker.md` | Updated | Added S47 entry |

## Decisions Made

| Decision | Rationale |
|----------|-----------|
| Three-pronged fix prioritized over single silver bullet | Each addresses a distinct root cause; combined effect should break the collapse cycle |
| Dual-criteria gate is CRITICAL (must implement first) | Without it, no amount of reward shaping helps — curriculum races past before evader can learn |
| PBRS weight increase from 10→50 not 100+ | 0.35/step is competitive with ±1.0 visibility without dominating; avoids reward scale issues |
| Defer NFSP/PSRO/PAIRED | Higher implementation complexity, simpler fixes should be tried first |

## Issues & Blockers

| Issue | Resolution |
|-------|------------|
| Run P collapsed same as H-O despite PBRS | Root cause is curriculum, not reward — PBRS can't help if evader never trains at obstacle levels |
| No PBRS-specific metrics in TensorBoard | Need to add reward decomposition logging in future runs |

## Next Steps
- [ ] Implement dual-criteria curriculum gate in `training/curriculum.py`
- [ ] Implement asymmetric training ratio in `training/amsdrl.py`
- [ ] Launch Run Q with all 3 fixes (dual-criteria + asymmetric + w=50)
- [ ] Add PBRS reward decomposition to TensorBoard logging
