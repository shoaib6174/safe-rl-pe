# Session 45 — 2026-02-24

## Context
- **Phase**: Phase 3 — Stage 3 PBRS obstacle-seeking implementation
- **Previous**: S44 researched PBRS solution for Level 3+ collapse. Wrote research report.

## Objectives
- [x] Implement PBRS obstacle-seeking in `envs/rewards.py`
- [x] Add obstacle distance tracking to `envs/pursuit_evasion_env.py`
- [x] Thread `w_obs_approach` through training pipeline
- [x] Add CLI arg `--w_obs_approach` to `scripts/train_amsdrl.py`
- [x] Write comprehensive tests for PBRS
- [x] Kill Run O and launch Run P on niro-2

## Work Done

### 1. PBRS in rewards.py
- Added `nearest_obstacle_distance(pos, obstacles)` helper function
  - Computes surface distance (center_dist - radius, clamped ≥ 0) to nearest obstacle
  - Returns `inf` when no obstacles present
- Added `w_obs_approach: float = 0.0` parameter to `RewardComputer.__init__`
- Added PBRS term in `RewardComputer.compute()`:
  - Accepts `d_obs_curr` and `d_obs_prev` kwargs
  - Computes `r_pbrs = w_obs_approach * (d_obs_prev - d_obs_curr) / d_max`
  - Applied only to evader reward, only on non-terminal steps
  - Combines additively with visibility reward (Mode B) when both active

### 2. Obstacle distance tracking in env
- Added `self.prev_d_nearest_obstacle` state variable
- Initialized in `reset()` using `nearest_obstacle_distance(evader_state, obstacles)`
- Updated in `step()`: compute `d_obs_curr`, pass both prev/curr to `reward_computer.compute()`, then update prev

### 3. Training pipeline threading
- Added `w_obs_approach` parameter to `_make_partial_obs_env()` signature
- Passed to `RewardComputer` constructor
- Added to `AMSDRLSelfPlay.__init__()` and `env_kwargs` dict
- Flows through `_make_vec_env()` via `**env_kwargs`

### 4. CLI arg
- Added `--w_obs_approach` to `scripts/train_amsdrl.py` with default 0.0
- Passed through to `AMSDRLSelfPlay` constructor

### 5. Tests (17 new tests)
- `TestNearestObstacleDistance` (7 tests): no obstacles, surface distance, multiple obstacles, inside obstacle, surface boundary, 3D state, diagonal
- `TestPBRSObstacleSeeking` (10 tests): approach positive, retreat negative, stationary zero, disabled w=0, no PBRS on capture/timeout, missing kwargs, combines with visibility, weight scales linearly, pursuer unaffected

All 53 reward tests pass. All 22 env tests pass. 575/648 total tests pass (70 failures are pre-existing BarrierNet/QP tests).

## Files Changed

| File | Action | Description |
|------|--------|-------------|
| `envs/rewards.py` | Modified | Added `nearest_obstacle_distance()`, `w_obs_approach` param, PBRS term in `compute()` |
| `envs/pursuit_evasion_env.py` | Modified | Added `prev_d_nearest_obstacle` tracking in `reset()` and `step()` |
| `training/amsdrl.py` | Modified | Added `w_obs_approach` to `_make_partial_obs_env()`, `AMSDRLSelfPlay.__init__()`, `env_kwargs` |
| `scripts/train_amsdrl.py` | Modified | Added `--w_obs_approach` CLI arg |
| `tests/test_rewards.py` | Modified | Added 17 tests (7 distance helper + 10 PBRS) |
| `docs/worklogs/2026-02-24_S45.md` | Created | This worklog |

## Decisions Made

| Decision | Rationale |
|----------|-----------|
| PBRS computed in `compute()` from pre-computed distances | Follows existing pattern (d_curr/d_prev passed from env) |
| `nearest_obstacle_distance` as module-level function | Reusable, testable, imported by both rewards.py and env |
| PBRS only affects evader reward | It's an evader-specific obstacle-seeking gradient |
| PBRS disabled on terminal steps | Standard PBRS practice — shaping on terminal states can distort |
| Use surface distance (not center) | Surface distance reaches 0 at obstacle, providing correct gradient |

## Issues & Blockers

| Issue | Resolution |
|-------|------------|
| `PursuitEvasionEnv.__init__()` got unexpected kwarg `w_obs_approach` in `_cold_start` | Added `"w_obs_approach"` to `_reward_keys` set that filters reward params before passing to env |
| Same error in `_build_callbacks` | Added `"w_obs_approach"` to `_rk` set in `_build_callbacks` |
| **NaN explosion** in Run P at Phase S2 — `ValueError: Expected parameter loc to satisfy Real()` | `nearest_obstacle_distance()` returns `inf` at L1/L2 (no obstacles). PBRS formula: `10.0 * (inf - inf) / d_max = NaN`. Fixed by adding `np.isfinite(d_obs_curr) and np.isfinite(d_obs_prev)` guards. Added test `test_no_pbrs_when_no_obstacles_inf`. |

## Run P Results

Run P launched on niro-2 (PID 255871, RTX 5090) with:
```
--w_obs_approach 10.0 --use_visibility_reward --survival_bonus 1.0 --prep_steps 100
--curriculum --opponent_pool_size 5 --max_phases 18 --seed 42
```

**Result: FAILED — Level 3+ collapse persists.** See S47 for detailed analysis.

- Curriculum raced L1→L6 in 5 phases (S3-S7)
- Evader never trained at L3 (close + obstacles) — level skipped
- First obstacle exposure at L4 (S6), pursuer already at 92% capture rate
- Final: SR_P=0.97, SR_E=0.03, NE gap=0.94, converged=False
- Total runtime: 2.3h

## Next Steps
- [x] ~~Kill Run O on niro-2~~ Done
- [x] ~~Launch Run P~~ Done, completed 18 phases
- [ ] Diagnose curriculum advancement root cause (→ S47)
- [ ] Implement dual-criteria curriculum gate + asymmetric training

## Metrics
- Files modified: 5
- Tests added: 17
- All tests pass (reward + env)
