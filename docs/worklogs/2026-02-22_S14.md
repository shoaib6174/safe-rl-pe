# Session 14 — 2026-02-22

## Context
- **Phase**: Phase 1: Simulation Foundation — Session 4 (Self-Play Training Loop + Health Monitor)
- **Branch**: main

## Objectives
- [x] Implement alternating self-play training loop
- [x] Implement head-to-head evaluation function
- [x] Implement trajectory collection for diversity analysis
- [x] Implement SelfPlayHealthMonitor (entropy, diversity, rollback, checkpoints)
- [x] Create self-play training script (scripts/train_self_play.py)
- [x] Add Hydra self-play config (conf/self_play/default.yaml)
- [x] Add 12 new tests (health monitor, eval, self-play smoke)

## Work Done

### Self-Play Training Loop (training/self_play.py)
- Vanilla alternating self-play: train pursuer (freeze evader) -> train evader (freeze pursuer)
- Opponent policy updated in vectorized env between phases
- Full history tracking (capture rate, escape rate, episode length, health alerts)
- Rolling model checkpoints

### Evaluation (training/self_play_eval.py)
- evaluate_both_agents(): head-to-head evaluation with deterministic actions
- collect_trajectories(): evader endpoint collection for diversity analysis

### Health Monitor (training/health_monitor.py)
- Entropy check via SB3's log_std parameter
- Trajectory diversity via distance-based endpoint clustering (no sklearn dependency)
- Capture rate balance check with rollback trigger
- Rolling checkpoint management with configurable max_checkpoints

### Testing
- 12 new tests across health monitor, eval, and self-play smoke
- Full self-play loop runs in ~3s in test mode
- All 59 tests pass

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `conf/self_play/default.yaml` | Created | Self-play Hydra config |
| `conf/config.yaml` | Modified | Added self_play defaults |
| `training/self_play.py` | Created | Alternating self-play loop |
| `training/self_play_eval.py` | Created | Head-to-head evaluation |
| `training/health_monitor.py` | Created | Health monitoring + checkpoints |
| `scripts/train_self_play.py` | Created | Self-play entry point |
| `tests/test_self_play.py` | Created | 12 self-play tests |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Distance-based clustering over K-means | Avoids sklearn dependency; simpler and sufficient for endpoint grouping |
| Entropy from log_std parameter | Direct access to SB3's Gaussian policy std, no rollout buffer parsing needed |
| Rolling checkpoints (max 5) | Prevents disk bloat during long self-play runs |

## Next Steps
- [ ] Session 5: Baseline implementations (DQN, DDPG, greedy) & comparison
