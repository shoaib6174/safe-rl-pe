# Session 41 — 2026-02-22

## Context
- **Phase**: Phase 3: Partial Observability — Session 3 (AMS-DRL Self-Play)
- **Continuation**: From Session 40 (training pipeline evaluation)

## Objectives
- [x] Implement PartialObsOpponentAdapter for self-play
- [x] Implement NavigationEnv for cold-start (S0)
- [x] Implement CheckpointManager with rolling + milestone support
- [x] Implement SB3 callbacks (entropy, health monitor, baseline eval)
- [x] Implement AMSDRLSelfPlay orchestrator
- [x] Implement NE verification tools and training script
- [x] Write comprehensive tests for all Session 3 components
- [x] Verify no regressions in existing tests

## Work Done

### 1. PartialObsOpponentAdapter
Key architectural problem: SingleAgentPEWrapper provides full-state observations to opponents, but Phase 3 policies expect partial-obs Dict observations. Created `PartialObsOpponentAdapter` that:
- Maintains its own FOV sensor, Lidar sensor, and observation history buffer
- Converts full-state obs to partial obs Dict format
- Matches SB3's `predict(obs) -> (action, state)` interface
- Is stateful — `reset()` called at episode boundaries

Also updated `SingleAgentPEWrapper.reset()` to call `opponent_policy.reset()` if available.

### 2. NavigationEnv (Cold-Start)
Created `NavigationEnv` for evader pre-training with two modes:
- **Goal-reaching**: Navigate to random goals with distance shaping + goal bonus
- **Flee**: Evade a slow scripted pursuer with survival + distance reward

Critical design: observation space matches PartialObsWrapper's Dict format exactly (`obs_history`, `lidar`, `state`) so policy weights transfer directly to self-play.

### 3. CheckpointManager
Full checkpoint management with:
- Rolling checkpoints (configurable max, auto-cleanup)
- Milestone checkpoints (per-phase, never auto-cleaned)
- Best-metric checkpoints
- Rollback support (loads both PPO + encoder)
- JSON metadata per checkpoint

### 4. SB3 Callbacks
Three callback classes for AMS-DRL health monitoring:
- **EntropyMonitorCallback**: Tracks entropy, clamps log_std floor to prevent collapse
- **SelfPlayHealthMonitorCallback**: Entropy + capture rate collapse/domination detection with automatic rollback
- **FixedBaselineEvalCallback**: Evaluates against scripted baselines with Elo tracking (dual frequency: lightweight + full)

Also implemented three scripted baseline policies: pure_pursuit, flee_away, flee_to_corner.

### 5. AMSDRLSelfPlay Orchestrator
Main self-play controller implementing the full AMS-DRL protocol:
- Phase S0: Cold-start evader with NavigationEnv
- Alternating phases: Train pursuer/evader with frozen opponent
- Per-phase evaluation with NE gap tracking
- Convergence detection (|SR_P - SR_E| < eta)
- Integrated health monitoring and checkpoint management
- Saves history, metrics, and final models

Key architecture: creates separate env stacks for pursuer/evader, uses PartialObsOpponentAdapter for frozen opponent integration.

### 6. NE Verification Tools
- `compute_ne_gap()`: NE gap from history
- `analyze_convergence()`: Convergence analysis with trend detection
- `plot_ne_convergence()`: Publication-quality NE convergence plot

### 7. Training Script
CLI script `train_amsdrl.py` with full argument support for Stage 3 (smoke test) and Stage 4 (full run).

### 8. Tests
38 tests covering all new components — all pass. Full suite: 474 passed (68 pre-existing QP/BarrierNet failures).

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `envs/opponent_adapter.py` | Created | PartialObsOpponentAdapter for self-play |
| `envs/navigation_env.py` | Created | NavigationEnv for cold-start (goal-reaching + flee) |
| `envs/wrappers.py` | Modified | Added adapter.reset() call in SingleAgentPEWrapper.reset() |
| `training/checkpoint_manager.py` | Created | CheckpointManager with rolling/milestone/best/rollback |
| `training/selfplay_callbacks.py` | Created | EntropyMonitor, HealthMonitor, BaselineEval callbacks + scripted baselines |
| `training/amsdrl.py` | Created | AMSDRLSelfPlay orchestrator + env factories |
| `training/ne_tools.py` | Created | NE gap computation, convergence analysis, plotting |
| `scripts/train_amsdrl.py` | Created | CLI training script for Stage 3/4 |
| `tests/test_session3.py` | Created | 38 tests for all Session 3 components |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Create PartialObsOpponentAdapter | SingleAgentPEWrapper provides full-state obs but frozen opponent expects partial-obs Dict. Adapter bridges this with its own sensor pipeline. |
| NavigationEnv obs matches PartialObsWrapper | Same Dict format ensures policy weights transfer directly from cold-start to self-play without architecture changes. |
| Separate checkpoint managers per agent | Pursuer and evader have independent rollback histories — rolling back one shouldn't affect the other. |
| Dual eval frequency for baselines | Lightweight eval (random only) every 10K steps, full eval (all baselines) every 50K steps — balances monitoring overhead. |
| No DCBF for evader | DCBF safety filter is pursuer's concern. Evader doesn't need collision avoidance constraints. |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| None | Clean implementation — all 38 tests pass on first run |

## Next Steps
- [ ] Deploy to niro-2 and run Stage 3 smoke test (3 phases, ~3-4h)
- [ ] Analyze Stage 3 Gate 3 criteria
- [ ] If Gate 3 passes, run Stage 4 full run (12 phases, ~8-15h)
- [ ] Update workflow tracker

## Metrics
- Files changed: 9
- Tests added: 38 (474 total passing)
- New classes: 7 (PartialObsOpponentAdapter, NavigationEnv, CheckpointManager, EntropyMonitorCallback, SelfPlayHealthMonitorCallback, FixedBaselineEvalCallback, AMSDRLSelfPlay)
- New functions: 9 (scripted baselines, env factories, NE tools)
