# Session 51 — 2026-02-25

## Context
- **Phase**: Phase 3: Partial Observability & Self-Play
- **Branch**: main

## Objectives
- [x] Add `w_wall` wall collision penalty parameter (mirrors `w_collision` pattern from S50)
- [x] Research collision/wall penalty design: literature review, relation to safe RL/CBF, calibration
- [x] Launch Run U with `w_wall=0.5, w_collision=0.5`
- [x] Monitor and triage Runs R, S, T

## Work Done

### `w_wall` Wall Collision Penalty
Added an optional per-agent wall collision penalty. Wall collisions are already physically enforced (`unicycle_step()` clips position), but there was no reward signal discouraging agents from pressing against walls. This mirrors the `w_collision` obstacle collision penalty added in S50.

- Default `0.0` for full backward compatibility
- Uses existing `p_wall`/`e_wall` booleans from `unicycle_step()` returns
- Per-agent penalty (not zero-sum), same pattern as `w_collision`

**Changes across 4 files:**
1. **`envs/pursuit_evasion_env.py`**: Added `w_wall` parameter to `__init__()`, stored as `self.w_wall`, added penalty block in `step()` after `w_collision`
2. **`training/amsdrl.py`**: Threaded `w_wall` through `_make_partial_obs_env()`, `AMSDRLSelfPlay.__init__()`, `_evaluate_head_to_head()`, `_evaluate_head_to_head_full_obs()`
3. **`scripts/train_amsdrl.py`**: Added `--w_wall` CLI argument, passed to `AMSDRLSelfPlay()`
4. **`tests/test_env.py`**: Added `TestWallPenalty` class with 2 tests

### Research: Collision & Wall Penalty Design
Comprehensive literature review on collision penalty magnitude, relation to CBF/safe RL, and calibration for our game setup. Reviewed 20+ papers from web search, 36 existing project papers, and 5 newly downloaded papers (N16–N20).

**Key findings:**
1. **Three paradigms**: reward-only, CBF-only, dual (CBF + reward). Dual is state-of-the-art.
2. **CBF + penalty is NOT redundant**: N17 (Yang et al. 2025) ablation proves dual approach (99.0%) outperforms filter-only (98.8%, drops to 38.7% without runtime filter) and reward-only (91.9%).
3. **Penalty magnitudes in literature**: obstacle/wall penalties range from -0.25 to -50, typically 0.5×–5× the goal reward. With CBFs active, smaller penalties suffice.
4. **Recommendation for our setup**: `w_wall = w_collision = 0.5` (14× per-step distance reward, 100 wall-steps = -50 = half of capture_bonus). Penalties >5.0 risk freezing behavior.

**5 new papers downloaded (N16–N20):**
- N16: Safe Finite-Time RL for PE (Kokolakis, CDC 2022) — barrier function (ψ=500) in PE cost
- N17: CBF-RL (Yang et al., 2025) — dual filter+reward, r_wall=-1.0, r_cbf weight=100
- N18: CBF-Safe Pursuit (Deng et al., 2024) — CBF-only QP filter, no penalty in reward
- N19: Large-Scale PE Collision Avoidance (Yang et al., IROS 2023) — f_c smooth proximity penalty + action masking
- N20: Survival Penalty Navigation (Jeng & Chiang, Sensors 2023) — argues against large penalties, uses episode concatenation

### Training Run Management

**Killed Runs R, S, T** — all collapsed at their respective curriculum levels:
- **Run R** (10×10, seed 43, min_escape_rate=0.05): Collapsed at L5. Advanced too fast due to lenient gate (0.05). SR_P=1.00, 8+ rollbacks. Killed at Phase S26.
- **Run S** (20×20, seed 44, min_escape_rate=0.15): Oscillating between L1/L2 at Phase S17. Not collapsed but not advancing either. Killed to free compute.
- **Run T** (10×10, seed 45, min_escape_rate=0.15): Collapsed at L2. SR_P=0.96–1.00, evader escape rate ~0% from Phase S15 onward. Killed at Phase S23.

**Launched Run U** (PID 1246077 on niro-2, RTX 5090):
- Config: Same as Run T + `w_wall=0.5, w_collision=0.5`, seed 46
- Synced S50+S51 code changes to remote (8 files: dynamics, env, rewards, observations, __init__, wrappers, amsdrl, train script)
- Early results (Phase S4, Level 1):

| Phase | SR_P | SR_E | NE Gap | Level |
|-------|------|------|--------|-------|
| S0 | 0.26 | 0.74 | — | Cold-start |
| S1 | 0.78 | 0.22 | 0.56 | L1 |
| S2 | 0.35 | 0.65 | 0.30 | L1 |
| S3 | 0.70 | 0.30 | 0.40 | L1 |
| S4 | — | — | — | L1 (training) |

**Final results (all 30 phases complete — COLLAPSED):**

| Phase | SR_P | SR_E | NE Gap | Level | Notes |
|-------|------|------|--------|-------|-------|
| S1–S4 | oscillating | — | 0.14–0.56 | L1 | Healthy start |
| S7 | 0.98 | 0.02 | 0.96 | L1 | Pursuer spike |
| S8 | 0.78 | 0.22 | 0.56 | L1→**L2** | Advanced |
| S10 | 0.69 | 0.31 | 0.38 | L2 | Evader recovering |
| S11 | 1.00 | 0.00 | 1.00 | L2 | **Collapsed** + rollback |
| S13 | 1.00 | 0.00 | 1.00 | **L1** | 6 rollbacks, dropped to L1 |
| S14–S30 | 0.92–1.00 | 0.00–0.08 | 0.84–1.00 | L1 | Permanently collapsed, 30+ rollbacks |

**Conclusion**: Wall/collision penalties (0.5) did NOT prevent collapse. Same L2 failure pattern as Runs R/S/T. The evader never recovered even after dropping back to L1. The core problem is a fundamental evader learning failure at curriculum transitions, not collision behavior.

## Files Changed
| File | Action | Description |
|------|--------|-------------|
| `envs/pursuit_evasion_env.py` | Modified | Added `w_wall` param, storage, and penalty logic in `step()` |
| `training/amsdrl.py` | Modified | Threaded `w_wall` through 4 functions |
| `scripts/train_amsdrl.py` | Modified | Added `--w_wall` CLI arg and passthrough |
| `tests/test_env.py` | Modified | Added `TestWallPenalty` class (2 tests) |
| `docs/research_collision_wall_penalty.md` | Created | Full research report with literature review, analysis, recommendations |
| `papers/supplementary/N16–N20` | Downloaded | 5 new papers on collision penalties & CBF-RL |

## Decisions Made
| Decision | Rationale |
|----------|-----------|
| Default `w_wall=0.0` | Backward compatibility — existing runs unaffected |
| Per-agent (not zero-sum) | Matches `w_collision` pattern; wall penalty is a shaping signal, not part of the game reward |
| Recommend `w_wall=w_collision=0.5` | 14× per-step distance reward; not so large as to cause freezing (literature warns >5.0 risks agent paralysis) |
| Kill Runs R, S, T | All collapsed or stagnating; freeing compute for Run U |

## Issues & Blockers
| Issue | Resolution |
|-------|------------|
| Remote missing S50 code (`resolve_obstacle_collisions`) | Synced 8 files via scp |
| Run R collapsed at L5 (min_escape_rate=0.05 too lenient) | Killed; future runs use 0.15 |
| Run T collapsed at L2 (same pattern as earlier runs) | Killed; Run U tests if wall/collision penalties help |

## Next Steps
- [ ] Investigate root cause of evader collapse at L2 — all 4 runs (R/S/T/U) exhibit the same pattern
- [ ] Consider structural evader improvements: architecture, reward, or training protocol changes
- [ ] Consider CBF-derived penalty (r_cbf from N17) as advanced alternative to fixed penalty

## Metrics
- Files changed: 6 (4 code + 1 report + 5 papers)
- Tests added: 2 (24 env tests total, 605 total passing)
- Papers downloaded: 5 (N16–N20)
- Runs killed: 3 (R, S, T — all collapsed)
- Runs completed: 1 (U — w_wall=0.5, w_collision=0.5 — collapsed at L2, same pattern)
- Commits: `e220a10`, `4c9e42d`, `008eddb`, `a45dc56`
