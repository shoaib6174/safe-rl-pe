# Session 58 — Status Report & Future Path

## Executive Summary

**The evader collapse in self-play is not a self-play dynamics problem — it's a fundamental learnability problem.** Through systematic diagnostic experiments, we proved that the evader cannot learn effective evasion at 1.05x speed advantage because proportional navigation guarantees capture in convex bounded arenas. However, when forced to equal speed with visibility-based obstacle rewards, the evader **does learn obstacle-hugging evasion** (peak 37% escape rate). This finding redirects our entire approach.

---

## 1. Self-Play Runs: RA9 & RA10 (Completed, Failed)

### Configuration
| Parameter | RA9 | RA10 |
|-----------|-----|------|
| Collapse rollback | threshold=0.05, streak=3 | threshold=0.05, streak=3 |
| PFSP-lite | enabled | enabled |
| Adaptive ratio | 0.3 / 20 phases | disabled |
| Survival bonus | 0.03 | 0.0 |

### Results (10M steps each)
| Metric | RA9 | RA10 |
|--------|-----|------|
| Rollback activations | **18** | **31** |
| Final SR_E | 0.000 | 0.000 |
| Pattern | Rollback → brief recovery → immediate re-collapse | Same, faster cycling |
| Best evader checkpoint | M50 (SR_E=0.760) | M50 (SR_E=0.880) |

### Diagnosis
The collapse rollback mechanism works correctly — it detects collapse and restores the best checkpoint. But the restored evader immediately collapses again because:
1. The opponent pool is filled with 50 strong pursuer checkpoints
2. Even PFSP-biased sampling toward older opponents isn't enough — all pool entries are strong
3. The fundamental issue: the evader's circular evasion strategy (learned early) breaks down once the pursuer learns to cut across

---

## 2. Diagnostic: Evader vs Fixed Greedy Pursuer

### The Key Question
Can the evader learn to evade **at all**, even against a fixed, predictable opponent?

### Experiment 1: Speed Advantage (1.05x, 2 obstacles)
**9 configurations tested** across arena sizes (10x10, 20x20), speed ratios (1.05x, 1.30x, 1.50x), and episode lengths (200-600 steps).

| Result | Escape Rate | Note |
|--------|-------------|------|
| All 9 configs | **0.000** | 2M training steps each |
| Avg episode length | ~80-85% of max | Evader survives long but never 100% |

**Root cause**: Proportional navigation (K_p=3.0) guarantees capture in bounded convex space regardless of speed advantage. The evader learned to run in circles (confirmed by trajectory visualization) but circles only delay — they don't prevent — capture.

### Trajectory Analysis (1.05x speed, trained model)

| Episodes | Outcome | Strategy |
|----------|---------|----------|
| 4/9 | **ESCAPED** (600 steps) | Large circular loops around arena perimeter |
| 5/9 | CAPTURED (78-223 steps) | Cornered before establishing circular pattern |

The evader **never uses obstacles** — purely speed-based open-field circles.

### Experiment 2: Equal Speed with Obstacle Incentives

| Run | Speed | Obstacles | Vis Weight | Peak Escape Rate | Final |
|-----|-------|-----------|------------|------------------|-------|
| D1 | 1.0x | **4** | 0.5 | 0.01 | **0.000** |
| D2 | 1.0x | **4** | 1.0 | 0.05 | **0.010** |
| D3 | 1.0x | **2** | 0.5 | **0.370** | 0.120 |

**D3 is the breakthrough.** At equal speed with 2 obstacles and visibility_weight=0.5:
- Escape rate rose steadily from 0% to **37% at 1.1M steps**
- Then gradually declined to 12% by 2M steps (policy degradation / entropy collapse)
- Average episode length: ~340 steps (57% of max)

**Why 4 obstacles failed**: Arena becomes too cluttered — obstacles trap the evader more than they help. The pursuer can corner the evader against obstacle clusters.

### Trajectory Analysis (D3: 1.0x speed, 2 obstacles)

| Behavior | Description |
|----------|-------------|
| **Obstacle-hugging orbits** | Evader circles tightly around a single obstacle, keeping it between itself and pursuer |
| **No open-field running** | Unlike 1.05x model — circular running is impossible at equal speed |
| **Cover exploitation** | Pursuer's proportional navigation drives it into the obstacle; evader stays on opposite side |
| **Failure mode** | Captured when evader can't reach an obstacle in time from spawn |

---

## 3. Root Cause Chain (Updated)

```
Original hypothesis: Self-play dynamics cause evader collapse
                          ↓
S57: Adaptive ratio + LR dampening → Failed (RA7b/RA8)
S58: Collapse rollback + PFSP → Failed (RA9/RA10)
                          ↓
    Diagnostic: evader vs fixed greedy pursuer
                          ↓
Finding: Evader CANNOT learn evasion at 1.05x speed
         (proportional navigation beats speed in bounded arena)
                          ↓
    Root cause: The evader has no viable strategy to learn
                          ↓
    Fix: Equal speed + obstacle incentives → evader learns obstacle cover
                          ↓
    D3: Peak 37% escape rate with obstacle-hugging behavior ✓
```

**The self-play collapse was a symptom, not the cause.** The evader collapsed because its only learned strategy (circular running) is inherently fragile — once the pursuer learns to cut across, the evader has no fallback.

---

## 4. Key Insights

### 4.1 Speed Advantage is a Trap
- Speed advantage makes open-field circles a viable strategy
- This is a local optimum that prevents the evader from discovering obstacle use
- When the pursuer eventually counters circles, the evader has no Plan B → collapse

### 4.2 Equal Speed Forces Obstacle Exploitation
- At equal speed, the only viable evasion strategy involves obstacles
- This forces the evader to learn the "correct" behavior (cover/occlusion)
- The resulting policy is more robust because it's based on geometry, not speed

### 4.3 Obstacle Count Matters
- **2 obstacles**: Enough cover options, arena remains navigable → 37% escape
- **4 obstacles**: Too cluttered, evader gets trapped between obstacles → 0% escape
- Sweet spot appears to be 2-3 obstacles in a 10x10 arena

### 4.4 Visibility Reward Works
- visibility_weight=0.5 with `line_of_sight_blocked` check provides clear signal
- Evader learns to position itself with obstacles between it and pursuer
- Weight 1.0 didn't help more (4-obstacle failure was the bottleneck, not reward strength)

---

## 5. Proposed Future Path

### Phase A: Stabilize D3 Configuration (Next Session)

**Problem**: D3 peaked at 37% then degraded to 12% over the next 900K steps.

**Likely cause**: Entropy collapse — the policy becomes overconfident in one obstacle-orbiting pattern and loses exploration. When episode seeds produce unfavorable obstacle placements, the rigid policy fails.

**Proposed fixes** (priority order):
1. **Early stopping**: Save best checkpoint (at ~1.1M steps), don't train to 2M
2. **Higher entropy coefficient**: Increase `ent_coef` from 0.01 to 0.02-0.05 to maintain exploration
3. **Entropy floor**: Add minimum entropy constraint to prevent policy collapse
4. **Longer training with learning rate schedule**: Cosine or linear decay LR to stabilize late training

**Experiment**: Train D3 config with `ent_coef=0.03` and LR schedule, save checkpoints every 100K, evaluate to find optimal training length.

### Phase B: Self-Play with Obstacle-Based Evader

Once the evader reliably learns obstacle evasion (~50%+ escape rate against greedy pursuer):

1. **Train pursuer against fixed obstacle-exploiting evader** — verify pursuer can learn to counter obstacle-hugging (go around obstacles instead of into them)
2. **Self-play with new config**: equal speed, 2 obstacles, visibility_weight=0.5, survival_bonus=0.03
3. **Keep collapse rollback + PFSP** — these mechanisms are correct, they just weren't enough when the evader had no viable strategy

### Phase C: Robustness & Generalization

1. **Randomize obstacle positions** (already done per episode) — verify policy generalizes across layouts
2. **Vary obstacle count** (1-3 per episode) — curriculum or uniform random
3. **Test against smarter pursuers** — obstacle-aware pursuit (RRT-based or learned)
4. **Partial observability** (Phase 3 goal) — limited FOV makes obstacle cover even more important

---

## 6. Files Changed This Session

| File | Action | Description |
|------|--------|-------------|
| `training/amsdrl.py` | Modified | Collapse rollback + PFSP integration |
| `training/opponent_pool.py` | Modified | `sample_pfsp()` method |
| `scripts/train_amsdrl.py` | Modified | 3 new CLI args |
| `scripts/train_evader_vs_greedy.py` | Modified | Refactored, fixed escape detection, added visibility_weight arg |
| `scripts/visualize_evader_vs_greedy.py` | Modified | Fixed state access, coordinate system, outcome detection |
| `scripts/visualize_trajectory_gif.py` | Created | Animated GIF generator |
| `tests/test_opponent_pool.py` | Modified | 7 new tests |

## 7. All Training Runs Summary

| Run | Steps | Escape Rate | Status | Key Finding |
|-----|-------|-------------|--------|-------------|
| RA9 | 10M | SR_E=0.00 | Dead | Rollback fires 18x, evader always re-collapses |
| RA10 | 10M | SR_E=0.00 | Dead | Rollback fires 31x, same pattern |
| Greedy (9 configs) | 2M each | 0.000 | Dead | Speed advantage doesn't enable escape |
| **D1** (4obs, vis=0.5) | 2M | 0.000 | Dead | 4 obstacles too cluttered |
| **D2** (4obs, vis=1.0) | 2M | 0.010 | Dead | Stronger reward can't fix clutter |
| **D3** (2obs, vis=0.5) | 2M | **0.370 peak** | **Best** | Obstacle-hugging evasion learned |

---

## 8. Immediate Next Steps

1. [ ] Train D3 config with higher entropy (ent_coef=0.03, 0.05) to prevent late-training degradation
2. [ ] Save checkpoints every 100K steps for early stopping analysis
3. [ ] Generate trajectory GIFs at peak performance (1.1M steps checkpoint)
4. [ ] If stable ≥40% escape: begin pursuer training against fixed obstacle-exploiting evader
5. [ ] If stable: launch self-play with equal speed + obstacle config
