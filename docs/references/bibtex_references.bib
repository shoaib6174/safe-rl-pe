% ============================================================
% BibTeX References for Safe RL + Pursuit-Evasion Research
% Pathway A: Safe Deep RL for 1v1 Ground Robot PE
% ============================================================

% ---- CORE PAPERS FROM ORIGINAL REVIEW ----

@article{wang2025rlsmccbf,
  title={Reinforcement Learning in Pursuit-Evasion Differential Game: Safety, Stability and Robustness},
  author={Wang, Xinyang and Zhang, Hongwei and Xu, Jun and Wang, Shimin and Guay, Martin},
  journal={arXiv preprint arXiv:2507.19516},
  year={2025},
  note={Paper [01]}
}

@article{gonultas2024carlike,
  title={1v1 Pursuit-Evasion on Car-like Ground Robots with Partial Observability},
  author={Gonultas, Burak and Isler, Volkan},
  year={2024},
  note={Paper [02] - F1TENTH/JetRacer deployment, BiMDN belief encoder}
}

@article{yang2025cbfrl,
  title={CBF-based Safety Filtering in Reinforcement Learning},
  author={Yang, Various},
  institution={Caltech},
  year={2025},
  note={Paper [05] - Closed-form CBF + reward shaping}
}

@article{emam2022robustcbf,
  title={Robust Control Barrier Functions with GP Disturbance Estimation},
  author={Emam, Various},
  year={2022},
  note={Paper [06] - RCBF-QP with GP for model uncertainty}
}

@inproceedings{bajcsy2024vision,
  title={Vision-based Pursuit-Evasion with Privileged Learning},
  author={Bajcsy, Andrea and others},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2024},
  note={Paper [08] - DAGGER distillation, Unitree A1}
}

@inproceedings{kokolakis2022safe,
  title={Safe Finite-Time Reinforcement Learning for Pursuit-Evasion Games},
  author={Kokolakis, Nick-Marios and Vamvoudakis, Kyriakos},
  booktitle={IEEE Conference on Decision and Control (CDC)},
  year={2022},
  note={Paper [13] - Barrier-augmented HJI cost}
}

@inproceedings{suttle2024cbfbeta,
  title={CBF-Constrained Beta Policies for Safe Reinforcement Learning},
  author={Suttle, Various},
  booktitle={AISTATS},
  year={2024},
  note={Paper [16] - Truncated policy convergence theorem}
}

@article{xiao2024amsdrl,
  title={AMS-DRL: Asynchronous Multi-Stage Self-Play for Deep Reinforcement Learning},
  author={Xiao, Various and Feroskhan, Various},
  year={2024},
  note={Paper [18] - NE convergence, real drone validation}
}

@article{teoh2025madr,
  title={MADR: Multi-Agent DeepReach for Pursuit-Evasion on TurtleBots},
  author={Teoh, Various},
  year={2025},
  note={Paper [22] - Game-optimal PE on physical robots}
}

@article{ganai2024hjrl,
  title={Hamilton-Jacobi Reachability and Reinforcement Learning: A Survey},
  author={Ganai, Various},
  year={2024},
  note={Paper [25] - Complete HJ+RL roadmap}
}

@article{lagatta2025shadow,
  title={SHADOW: Information-Aware Pursuit-Evasion with Unicycle Dynamics},
  author={La Gatta, Various},
  year={2025},
  note={Paper [34] - Multi-headed TD3+PPO, PEEC formulation}
}

@article{salimpour2025sim2real,
  title={Sim-to-Real Transfer for Mobile Robots: Isaac Sim to Gazebo to Real ROS2 Robots},
  author={Salimpour, Various},
  journal={arXiv preprint arXiv:2501.02902},
  year={2025},
  note={Paper [07]}
}

% ---- NEWLY IDENTIFIED PAPERS ----

@article{guerrier2024cbfsurvey,
  title={Learning Control Barrier Functions and their Application in Reinforcement Learning: A Survey},
  author={Guerrier, Maeva and Fouad, Hassan and Beltrame, Giovanni},
  journal={arXiv preprint arXiv:2404.16879},
  year={2024},
  note={N01 - Survey of data-driven CBF learning}
}

@article{so2024pncbf,
  title={How to Train Your Neural Control Barrier Function: Learning Safety Filters for Complex Input-Constrained Systems},
  author={So, Oswin and Serlin, Zachary and Mann, Makai and Gonzales, Jake and Rutledge, Kwesi and Roy, Nicholas and Fan, Chuchu},
  journal={arXiv preprint arXiv:2310.15478},
  year={2024},
  note={N02 - PNCBF: value function of max-over-time cost is a CBF}
}

@article{gu2022macpo,
  title={Multi-Agent Constrained Policy Optimisation},
  author={Gu, Shangding and Kuba, Jakub Grudzien and Wen, Munning and Chen, Ruiqing and Wang, Ziyan and Tian, Zheng and Wang, Jun and Knoll, Alois and Yang, Yaodong},
  journal={arXiv preprint arXiv:2110.02793},
  year={2022},
  note={N03 - MACPO/MAPPO-Lagrangian with constraint guarantees}
}

@article{xiao2023barriernet,
  title={BarrierNet: Differentiable Control Barrier Functions for Learning of Safe Robot Control},
  author={Xiao, Wei and Wang, Tsun-Hsuan and Hasani, Ramin and Chahine, Makram and Amini, Alexander and Li, Xiao and Rus, Daniela},
  journal={IEEE Transactions on Robotics},
  volume={39},
  number={3},
  pages={2289--2307},
  year={2023},
  note={N04 - End-to-end trainable dCBFs in differentiable QPs}
}

@article{zhang2024gcbfplus,
  title={GCBF+: A Neural Graph Control Barrier Function Framework for Distributed Safe Multi-Agent Control},
  author={Zhang, Songyuan and So, Oswin and Garg, Kunal and Fan, Chuchu},
  journal={IEEE Transactions on Robotics},
  year={2024},
  note={N05 - Graph-structured CBFs, scales to 1024 agents, drone validation}
}

@article{benjabeur2025roboticpe,
  title={Robotic Pursuit Evasion Problem in a Constrained Game Area using Deep Reinforcement Learning and Self-Play Training},
  author={Ben Jabeur, Chiraz and Seddik, Hassene and Khnissi, Khaled and Hably, Ahmad},
  journal={Research Square preprint rs-6279213/v1},
  year={2025},
  note={N06 - TD3 self-play for 1v1 PE, no safety; useful baseline}
}

@inproceedings{bouvier2024policed,
  title={POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable Satisfaction of Hard Constraints},
  author={Bouvier, Jean-Baptiste and Nagpal, Kartik and Mehr, Negar},
  booktitle={Robotics: Science and Systems (RSS)},
  year={2024},
  note={N07 - Affine policy near boundary for provable constraint satisfaction}
}

@article{jin2024statewise,
  title={Constrained Reinforcement Learning with Statewise Projection: A Control Barrier Function Approach},
  author={Jin, X. and Li, K. and Jia, Q.},
  journal={Science China Information Sciences},
  year={2024},
  note={N08 - CBF projection-based safe RL}
}

@article{saferl2025survey,
  title={A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety},
  journal={arXiv preprint arXiv:2505.17342},
  year={2025},
  note={N09 - Comprehensive safe RL + constrained MDP survey}
}

@article{salimpour2025isaac,
  title={Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots},
  author={Salimpour, Various},
  journal={arXiv preprint arXiv:2501.02902},
  year={2025},
  note={N10 - Full Isaac Sim pipeline validation}
}

@article{isaaclabpaper2025,
  title={Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning},
  journal={arXiv preprint arXiv:2511.04831},
  year={2025},
  note={N11 - Isaac Lab framework details}
}

@article{zhang2025cbfclmr,
  title={Dynamic obstacle avoidance for car-like mobile robots based on neurodynamic optimization with control barrier functions},
  author={Zhang, Zheng and Yang, Guang-Hong},
  journal={Neurocomputing},
  volume={654},
  pages={131252},
  year={2025},
  publisher={Elsevier},
  doi={10.1016/j.neucom.2025.131252},
  note={N12 - Virtual control point CBF + M-matrix auxiliary input for nonholonomic robots; uniform relative degree 1}
}

@article{sbtrpo2024,
  title={Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions},
  journal={arXiv preprint arXiv:2512.23770},
  year={2024},
  note={N14 - Trust-region hard-constrained RL}
}

@article{liu2025rmarlcbfsam,
  title={Safe robust multi-agent reinforcement learning with neural control barrier functions and safety attention mechanism},
  author={Liu, Shihan and Liu, Lijun and Yu, Zhen},
  journal={Information Sciences},
  volume={690},
  pages={121567},
  year={2025},
  publisher={Elsevier},
  doi={10.1016/j.ins.2024.121567},
  note={N15 - RMARL-CBF-SAM: H-infinity robust MARL + decentralized neural CBFs + safety attention mechanism}
}

% ---- SELF-PLAY, CURRICULUM, AND MULTI-AGENT TRAINING ----
% Added in S47 (2026-02-24) for Level 3+ collapse analysis

@inproceedings{baker2020emergent,
  title={Emergent Tool Use from Multi-Agent Autocurricula},
  author={Baker, Bowen and Kanitscheider, Ingmar and Marber, Todor and Epstein, Max and Gordon, Bryan and Paine, Thomas L and Mordatch, Igor and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020},
  url={https://arxiv.org/abs/1909.07528},
  note={OpenAI Hide-and-Seek: emergent tool use via zero-sum reward + massive scale (31.7B frames). Preparation phase gives hiders head start. 6 strategy phases emerged.}
}

@inproceedings{bansal2018emergent,
  title={Emergent Complexity via Multi-Agent Competition},
  author={Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018},
  url={https://arxiv.org/abs/1710.03748},
  note={Multi-agent competition in MuJoCo. Alternating freeze training with opponent sampling. Emergent complexity in sumo/soccer/kick-and-defend.}
}

@inproceedings{rao2024masp,
  title={A Multi-party Asymmetric Self-play Algorithm},
  author={Rao, Yongzhao and others},
  booktitle={International Conference on Machine Learning and Principles and Practice of Knowledge Discovery (MLPRAE)},
  year={2024},
  url={https://dl.acm.org/doi/10.1145/3696687.3696712},
  note={MASP: ELO-based dynamic adjustment of training update frequency. Stronger agent gets fewer updates, weaker agent gets more.}
}

@article{czempin2022reducing,
  title={Reducing Exploitability with Population Based Training},
  author={Czempin, Patrick and Gleave, Adam},
  journal={arXiv preprint arXiv:2208.05083},
  year={2022},
  url={https://arxiv.org/abs/2208.05083},
  note={Robustness scales with population size. RPPO (reward-randomized PPO) ensures population diversity. Prevents overfitting to single opponent.}
}

@article{vinyals2019alphastar,
  title={Grandmaster level in {StarCraft II} using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group},
  note={AlphaStar league: main agents, main exploiters, league exploiters. PFSP opponent selection. 44-day training, 900+ agents.}
}

@inproceedings{dennis2020paired,
  title={Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design},
  author={Dennis, Michael and Jaques, Natasha and Vinitsky, Eugene and Baez, Alexandre and Singh, Simone and Levine, Sergey and Dru, Russell},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020},
  url={https://arxiv.org/abs/2012.02096},
  note={PAIRED: protagonist-antagonist-adversary. Adversary generates environments to maximize regret. Naturally produces curriculum at boundary of agent's capability.}
}

@inproceedings{jiang2021replay,
  title={Replay-Guided Adversarial Environment Design},
  author={Jiang, Minqi and Dennis, Michael and Parker-Holder, Jack and Foerster, Jakob and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2021},
  url={https://arxiv.org/abs/2110.02439},
  note={Extends PAIRED with experience replay for environment design. Improves sample efficiency.}
}

@article{narvekar2020curriculum,
  title={Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey},
  author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research (JMLR)},
  volume={21},
  number={181},
  pages={1--50},
  year={2020},
  note={Comprehensive survey of curriculum learning for RL. Framework for task sequencing, transfer, and evaluation.}
}

@inproceedings{heinrich2015fictitious,
  title={Fictitious Self-Play in Extensive-Form Games},
  author={Heinrich, Johannes and Lanctot, Marc and Silver, David},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2015},
  url={https://proceedings.mlr.press/v37/heinrich15.html},
  note={FSP: best-respond to opponent's average policy (mixture of all historical policies). Converges to NE in two-player zero-sum.}
}

@article{heinrich2016nfsp,
  title={Deep Reinforcement Learning from Self-Play in Imperfect-Information Games},
  author={Heinrich, Johannes and Silver, David},
  journal={arXiv preprint arXiv:1603.01121},
  year={2016},
  url={https://arxiv.org/abs/1603.01121},
  note={NFSP: neural implementation of FSP. Two networks per agent â€” best response (RL) + average policy (supervised on reservoir buffer). Prevents cyclic strategies.}
}

@inproceedings{lanctot2017psro,
  title={A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
  author={Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and P{\'e}rolat, Julien and Silver, David and Graepel, Thore},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2017},
  url={https://mlanctot.info/files/papers/nips17-psro.pdf},
  note={PSRO: maintain policy population, compute meta-game payoff matrix, solve Nash equilibrium, train best response, add to population. Unifies FSP, DO, and EGTA.}
}

@article{mcaleer2022selfplay,
  title={Self-Play {PSRO}: Toward Optimal Populations in Two-Player Zero-Sum Games},
  author={McAleer, Stephen and Lanier, John B and Fox, Roy and Baldi, Pierre},
  journal={arXiv preprint arXiv:2207.06541},
  year={2022},
  url={https://arxiv.org/abs/2207.06541},
  note={SP-PSRO: incorporates self-play into PSRO for faster convergence in two-player zero-sum.}
}

@inproceedings{lockhart2019exploitability,
  title={Computing Approximate Equilibria in Sequential Adversarial Games by Exploitability Descent},
  author={Lockhart, Edward and Lanctot, Marc and P{\'e}rolat, Julien and Lespiau, Jean-Baptiste and Morrill, Dustin and Timbers, Finbarr and Tuyls, Karl},
  booktitle={International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2019},
  url={https://arxiv.org/abs/1903.05614},
  note={Exploitability descent: directly optimize exploitability via gradient descent. Both players independently minimize NashConv. Converges actual policies (not averages).}
}

@inproceedings{timbers2022approximate,
  title={Approximate Exploitability: Learning a Best Response in Large Games},
  author={Timbers, Finbarr and Lockhart, Edward and Schmid, Martin and Lanctot, Marc and Bowling, Michael},
  booktitle={International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2022},
  note={Learned approximate best response makes exploitability-based methods practical for large games.}
}

@inproceedings{balcan2023nash,
  title={Nash Equilibria and Pitfalls of Adversarial Training in Adversarial Robustness Games},
  author={Balcan, Maria-Florina and others},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2023},
  url={https://arxiv.org/abs/2210.12606},
  note={Theoretical analysis of adversarial training dynamics, NE existence, and convergence pitfalls.}
}

@inproceedings{huang2023robust,
  title={A Robust and Opponent-Aware League Training Method for {StarCraft II}},
  author={Huang, Ruozi and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  note={Opponent-aware league training. Improves on AlphaStar league with adaptive matchmaking.}
}

@article{lanctot2019openspiel,
  title={{OpenSpiel}: A Framework for Reinforcement Learning in Games},
  author={Lanctot, Marc and Lockhart, Edward and Lespiau, Jean-Baptiste and Zambaldi, Vinicius and Upadhyay, Satyaki and P{\'e}rolat, Julien and Srinivasan, Sriram and Timbers, Finbarr and Tuyls, Karl and Omidshafiei, Shayegan and others},
  journal={arXiv preprint arXiv:1908.09453},
  year={2019},
  note={Reference implementations of FSP, NFSP, PSRO, and other multi-agent RL algorithms.}
}

@inproceedings{aaai2024subgame,
  title={Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2024},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/29011},
  note={Adaptive subgame sampling based on difficulty for zero-sum MARL. Accelerates convergence by focusing on subgames at agent's capability boundary.}
}

@article{selfplay2024survey,
  title={A Survey on Self-play Methods in Reinforcement Learning},
  journal={arXiv preprint arXiv:2408.01072},
  year={2024},
  note={Comprehensive survey of self-play methods: vanilla, FSP, PSRO, population-based, league training.}
}

% ---- REWARD SHAPING (PBRS) ----
% Referenced in S44-S45 for obstacle-seeking implementation

@inproceedings{ng1999pbrs,
  title={Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={International Conference on Machine Learning (ICML)},
  year={1999},
  note={PBRS: F(s,a,s') = gamma*Phi(s') - Phi(s) is the ONLY additive shaping that preserves optimal policy. Foundation for reward shaping theory.}
}

@article{devlin2012pbrs,
  title={Dynamic Potential-Based Reward Shaping},
  author={Devlin, Sam and Kudenko, Daniel},
  journal={Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  year={2012},
  note={Extends PBRS to multi-agent settings. Shows PBRS preserves Nash equilibria in competitive games.}
}
