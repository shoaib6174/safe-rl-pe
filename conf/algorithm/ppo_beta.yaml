# PPO with Beta distribution policy
# Same hyperparameters as Gaussian PPO, but uses BetaPolicy
# for naturally bounded actions (no clipping artifacts)
learning_rate: 3.0e-4
n_steps: 512
batch_size: 256
n_epochs: 10
gamma: 0.99
gae_lambda: 0.95
clip_range: 0.2
ent_coef: 0.01
vf_coef: 0.5
max_grad_norm: 0.5
policy_kwargs:
  net_arch:
    - 256
    - 256
# Note: BetaPolicy is passed via policy_class in train.py, not here
